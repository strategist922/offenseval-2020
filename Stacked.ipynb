{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data in 0.07s\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, vstack, hstack\n",
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "training_data = 'data/OLIDv1.0/olid-training-v1.0.tsv'\n",
    "test_data = 'data/OLIDv1.0/testset-levela.tsv'\n",
    "test_labels = 'data/OLIDv1.0/labels-levela.csv'\n",
    "hashtags = 'data/olid_segmentations.tsv'\n",
    "\n",
    "np.random.seed(1234) # help reproducibility\n",
    "\n",
    "start = time()    \n",
    "with open(test_data, encoding='utf-8') as f:\n",
    "    raw = csv.reader(f, delimiter='\\t')\n",
    "    test_ids = []\n",
    "    x_test_raw = []\n",
    "    for r in raw:\n",
    "        test_ids.append(r[0])\n",
    "        x_test_raw.append(r[1])\n",
    "    test_ids = [int(i) for i in test_ids[1:]]\n",
    "    x_test_raw = x_test_raw[1:]\n",
    "        \n",
    "with open(test_labels, encoding='utf-8') as f:\n",
    "    raw = csv.reader(f, delimiter=',')\n",
    "    y = []\n",
    "    for r in raw:\n",
    "        y.append(0 if r[1] == 'NOT' else 1)\n",
    "\n",
    "with open('predictions/david-lr-val.txt') as f:\n",
    "    y_dlr = np.array([int(line) for line in f])\n",
    "    y_dlr = np.expand_dims(y_lr, axis=1)\n",
    "    \n",
    "with open('predictions/jp-lr-val.txt') as f:\n",
    "    y_jlr = np.array([int(line) for line in f])\n",
    "    y_jlr = np.expand_dims(y_lr, axis=1)\n",
    "    \n",
    "with open('predictions/svm-val.txt') as f:\n",
    "    y_svm = np.array([int(line) for line in f])\n",
    "    y_svm = np.expand_dims(y_svm, axis=1)\n",
    "   \n",
    "with open('predictions/xgb-val.txt') as f:\n",
    "    y_xgb = np.array([int(line) for line in f])\n",
    "    y_xgb = np.expand_dims(y_xgb, axis=1)\n",
    "    \n",
    "with open('predictions/cnn-val.txt') as f:\n",
    "    y_cnn = np.array([int(line) for line in f])\n",
    "    y_cnn = np.expand_dims(y_xgb, axis=1)\n",
    "\n",
    "# Load hashtag segmentations\n",
    "segmentations = {}\n",
    "for line in open(hashtags, encoding='utf-8'):\n",
    "    terms = [x.strip().lower() for x in line.split('\\t')]\n",
    "    hashtag, segmentation = terms[0], terms[1]\n",
    "    segmentations[hashtag] = segmentation\n",
    "print('Loaded data in %.2fs' % (time() - start))\n",
    "\n",
    "def shuffle_together(x, y):\n",
    "    # Shuffle x and y together\n",
    "    i = np.arange(x.shape[0])\n",
    "    np.random.shuffle(i)\n",
    "    return x[i, :], y[i] # shuffling a sparse matrix is weird\n",
    "\n",
    "def report(y, y_hat, metrics=['accuracy', 'precision', 'recall', 'f1', 'auc']):\n",
    "    results = []\n",
    "    if 'accuracy' in metrics or 'acc' in metrics:\n",
    "        results.append(skmetrics.accuracy_score(y, y_hat))\n",
    "    if 'precision' in metrics:\n",
    "        results.append(skmetrics.precision_score(y, y_hat))\n",
    "    if 'recall' in metrics:\n",
    "        results.append(skmetrics.recall_score(y, y_hat))\n",
    "    if 'f1' in metrics:\n",
    "        results.append(skmetrics.f1_score(y, y_hat, average='weighted'))\n",
    "    if 'auc' in metrics:\n",
    "        results.append(skmetrics.f1_score(y, y_hat, average='macro'))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False)\n",
    "class Wrapper:\n",
    "    def __init__(self, tweet_tk, segmentations):\n",
    "        self.tweet_tk = tweet_tk\n",
    "        self.segmentations = segmentations\n",
    "    \n",
    "    def tokenize(self, x):\n",
    "        tokens = []\n",
    "        for token in self.tweet_tk.tokenize(x):\n",
    "            if token[0] == '#' and token[1:] in self.segmentations:\n",
    "                sequence = self.segmentations[token[1:]].split()\n",
    "            else:\n",
    "                sequence = [token]\n",
    "\n",
    "            for word in sequence:\n",
    "                tokens.append(word)\n",
    "        return tokens\n",
    "tk = Wrapper(tokenizer, segmentations)\n",
    "          \n",
    "vectorizer = TfidfVectorizer(tokenizer=tk.tokenize, \n",
    "                             strip_accents='unicode', \n",
    "                             lowercase=True,\n",
    "                             sublinear_tf=True,\n",
    "                             min_df=9,\n",
    "                             stop_words='english'\n",
    "                            )\n",
    "\n",
    "x = vectorizer.fit_transform(x_raw)\n",
    "x = csr_matrix(hstack((x, y_lr, y_svm, y_xgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "\n",
      "SVM\n",
      "\n",
      "XGB\n",
      "md=3, lr=0.012000, n=100\n",
      "acc     p       r       f1w     f1m     time    \n",
      "0.7628  0.6833  0.5516  0.7557  0.7200  1.65s\n",
      "0.7742  0.7057  0.5576  0.7666  0.7309  1.68s\n",
      "0.7742  0.7536  0.5485  0.7643  0.7357  1.69s\n",
      "0.7742  0.6624  0.5150  0.7652  0.7126  1.66s\n",
      "0.7772  0.7209  0.5353  0.7674  0.7289  2.01s\n",
      "0.7847  0.7241  0.5714  0.7774  0.7427  1.90s\n",
      "0.7847  0.7072  0.5701  0.7780  0.7397  1.79s\n",
      "0.7674  0.7222  0.5177  0.7561  0.7193  1.72s\n",
      "0.7689  0.6877  0.5492  0.7614  0.7232  1.74s\n",
      "0.7734  0.6902  0.5773  0.7678  0.7328  1.61s\n",
      "average:\n",
      "0.7742  0.7057  0.5494  0.7660  0.7286  1.75s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.sparse import vstack\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def test(model):\n",
    "    k = 10\n",
    "    kf = KFold(n_splits=k)\n",
    "    average_acc = 0\n",
    "    print(6 * '%-8s' % ('acc', 'p', 'r', 'f1w', 'f1m', 'time'))\n",
    "    averages = np.array([0] * 6, dtype='float')\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        start = time()\n",
    "        # Split based on k-fold\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        x_train, y_train = shuffle_together(x_train, y_train)\n",
    "        clf = model.fit(x_train, y_train)\n",
    "        y_hat = clf.predict(x_test)\n",
    "        vals = report(y_test, y_hat) + [time() - start]\n",
    "        averages += vals\n",
    "        print('%.4f  %.4f  %.4f  %.4f  %.4f  %.2fs' % tuple(vals))\n",
    "    print('average:')\n",
    "    averages /= k\n",
    "    print('%.4f  %.4f  %.4f  %.4f  %.4f  %.2fs' % tuple(averages))\n",
    "    \n",
    "print('XGB')\n",
    "# 3, 0.003 and 0.012, 100 = 0.7286\n",
    "# 3, .01, 100  = .7282\n",
    "# 4, .01, 100  = .7257\n",
    "for md in [3]:\n",
    "    for lr in [.012]:\n",
    "        n = 100\n",
    "        print('md=%d, lr=%f, n=%d' % (md, lr, n))\n",
    "        model = XGBClassifier(max_depth=md, learning_rate=lr, n_estimators=n)\n",
    "        test(model)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
