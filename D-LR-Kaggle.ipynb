{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misspellings of curse words?  \n",
    "\n",
    "When building vocab, consider punctuation. Tokenize it first? Right now it just splits on whitespace. Moses probably has something for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "np.random.seed(1234) # help reproducibility\n",
    "data = 'data/jigsaw/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "olid = open(data)\n",
    "df = pd.read_csv(olid, sep=',')\n",
    "olid.close()\n",
    "df = df.drop('id', 1)\n",
    "\n",
    "#make a single column from the 6 types of offensive in the data\n",
    "df['offensive'] = df[df.columns[1:]].apply(\n",
    "    lambda x: ','.join(x.dropna().astype(str)),\n",
    "    axis=1)\n",
    "df = df.drop(df.columns[[1,2,3,4,5,6]],1)\n",
    "\n",
    "#make the single offensive column 0 if all elements are 0 and 1 if any are 1.\n",
    "df['offensive'] = df['offensive'].map(lambda x: 0 if all(y=='0' for y in x.split(',')) else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        comment_text  offensive\n",
      "0  Explanation\\nWhy the edits made under my usern...          0\n",
      "1  D'aww! He matches this background colour I'm s...          0\n",
      "2  Hey man, I'm really not trying to edit war. It...          0\n",
      "3  \"\\nMore\\nI can't make any real suggestions on ...          0\n",
      "4  You, sir, are my hero. Any chance you remember...          0\n"
     ]
    }
   ],
   "source": [
    "data_top = df.head()\n",
    "print(data_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = np.array(df['comment_text'])\n",
    "offen = np.array(df['offensive'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "text_train, text_test, offen_train, offen_test = train_test_split(text, offen, test_size=0.2, random_state=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create word count and tf-idf features for the training and test sets\n",
    "cv = CountVectorizer(stop_words='english',max_features=30000)\n",
    "tv = TfidfVectorizer(min_df=0.0, max_df=0.80, sublinear_tf=True)\n",
    "\n",
    "cv_text_features = cv.fit_transform(text_train)\n",
    "tv_text_features = tv.fit_transform(text_train)\n",
    "\n",
    "cv_test_features = cv.transform(text_test)\n",
    "tv_test_features = tv.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "combinedFeatures = FeatureUnion([\n",
    "    ('bow' , CountVectorizer(stop_words='english',max_features=30000)),\n",
    "    ('tf-idf', TfidfVectorizer(min_df=0.0, max_df=0.80, sublinear_tf=True))])\n",
    "\n",
    "text_features = combinedFeatures.fit_transform(text_train)\n",
    "test_features = combinedFeatures.transform(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.9567\n",
      "Precision: 0.9548\n",
      "Recall: 0.9567\n",
      "F1 Score: 0.9537\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     28657\n",
      "           1       0.89      0.66      0.76      3258\n",
      "\n",
      "    accuracy                           0.96     31915\n",
      "   macro avg       0.93      0.82      0.87     31915\n",
      "weighted avg       0.95      0.96      0.95     31915\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "          Predicted:      \n",
      "                   0     1\n",
      "Actual: 0      28393   264\n",
      "        1       1118  2140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.6/site-packages/ipykernel_launcher.py:58: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "/home/david/.local/lib/python3.6/site-packages/ipykernel_launcher.py:60: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model on TF-IDF features\n",
    "lr_tfidf_predictions = train_predict_model(classifier=lr, train_features=text_features, train_labels=offen_train,\n",
    "                                           test_features=test_features, test_labels=offen_test)\n",
    "\n",
    "display_model_performance_metrics(true_labels=offen_test, predicted_labels=lr_tfidf_predictions,\n",
    "                                      classes=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.9528\n",
      "Precision: 0.9506\n",
      "Recall: 0.9528\n",
      "F1 Score: 0.949\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97     28657\n",
      "           1       0.89      0.62      0.73      3258\n",
      "\n",
      "    accuracy                           0.95     31915\n",
      "   macro avg       0.92      0.80      0.85     31915\n",
      "weighted avg       0.95      0.95      0.95     31915\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "          Predicted:      \n",
      "                   0     1\n",
      "Actual: 0      28402   255\n",
      "        1       1251  2007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.6/site-packages/ipykernel_launcher.py:58: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "/home/david/.local/lib/python3.6/site-packages/ipykernel_launcher.py:60: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model on TF-IDF features\n",
    "lr_tfidf_predictions = train_predict_model(classifier=lr, train_features=cv_text_features, train_labels=offen_train,\n",
    "                                           test_features=cv_test_features, test_labels=offen_test)\n",
    "\n",
    "display_model_performance_metrics(true_labels=offen_test, predicted_labels=lr_tfidf_predictions,\n",
    "                                      classes=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Evaluation metrics\n",
    "## I copied this from the article\n",
    "## https://www.kaggle.com/divsinha/sentiment-analysis-countvectorizer-tf-idf\n",
    "\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve, auc \n",
    "\n",
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    \n",
    "    print('Accuracy:', np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        4))\n",
    "    print('Precision:', np.round(\n",
    "                        metrics.precision_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "    print('Recall:', np.round(\n",
    "                        metrics.recall_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "    print('F1 Score:', np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "                        \n",
    "\n",
    "def train_predict_model(classifier, \n",
    "                        train_features, train_labels, \n",
    "                        test_features, test_labels):\n",
    "    # build model    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features) \n",
    "    return predictions    \n",
    "\n",
    "\n",
    "def display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n",
    "    \n",
    "    total_classes = len(classes)\n",
    "    level_labels = [total_classes*[0], list(range(total_classes))]\n",
    "\n",
    "    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n",
    "                                  labels=classes)\n",
    "    cm_frame = pd.DataFrame(data=cm, \n",
    "                            columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n",
    "                                                  labels=level_labels), \n",
    "                            index=pd.MultiIndex(levels=[['Actual:'], classes], \n",
    "                                                labels=level_labels)) \n",
    "    print(cm_frame) \n",
    "    \n",
    "def display_classification_report(true_labels, predicted_labels, classes=[1,0]):\n",
    "\n",
    "    report = metrics.classification_report(y_true=true_labels, \n",
    "                                           y_pred=predicted_labels, \n",
    "                                           labels=classes) \n",
    "    print(report)\n",
    "    \n",
    "    \n",
    "    \n",
    "def display_model_performance_metrics(true_labels, predicted_labels, classes=[1,0]):\n",
    "    print('Model Performance metrics:')\n",
    "    print('-'*30)\n",
    "    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n",
    "    print('\\nModel Classification report:')\n",
    "    print('-'*30)\n",
    "    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, \n",
    "                                  classes=classes)\n",
    "    print('\\nPrediction Confusion Matrix:')\n",
    "    print('-'*30)\n",
    "    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n",
    "                             classes=classes)\n",
    "\n",
    "\n",
    "def plot_model_decision_surface(clf, train_features, train_labels,\n",
    "                                plot_step=0.02, cmap=plt.cm.RdYlBu,\n",
    "                                markers=None, alphas=None, colors=None):\n",
    "    \n",
    "    if train_features.shape[1] != 2:\n",
    "        raise ValueError(\"X_train should have exactly 2 columnns!\")\n",
    "    \n",
    "    x_min, x_max = train_features[:, 0].min() - plot_step, train_features[:, 0].max() + plot_step\n",
    "    y_min, y_max = train_features[:, 1].min() - plot_step, train_features[:, 1].max() + plot_step\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    clf_est = clone(clf)\n",
    "    clf_est.fit(train_features,train_labels)\n",
    "    if hasattr(clf_est, 'predict_proba'):\n",
    "        Z = clf_est.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "    else:\n",
    "        Z = clf_est.predict(np.c_[xx.ravel(), yy.ravel()])    \n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=cmap)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(train_labels)\n",
    "    n_classes = len(le.classes_)\n",
    "    plot_colors = ''.join(colors) if colors else [None] * n_classes\n",
    "    label_names = le.classes_\n",
    "    markers = markers if markers else [None] * n_classes\n",
    "    alphas = alphas if alphas else [None] * n_classes\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y_enc == i)\n",
    "        plt.scatter(train_features[idx, 0], train_features[idx, 1], c=color,\n",
    "                    label=label_names[i], cmap=cmap, edgecolors='black', \n",
    "                    marker=markers[i], alpha=alphas[i])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n",
    "    \n",
    "    ## Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    if hasattr(clf, 'classes_'):\n",
    "        class_labels = clf.classes_\n",
    "    elif label_encoder:\n",
    "        class_labels = label_encoder.classes_\n",
    "    elif class_names:\n",
    "        class_labels = class_names\n",
    "    else:\n",
    "        raise ValueError('Unable to derive prediction classes, please specify class_names!')\n",
    "    n_classes = len(class_labels)\n",
    "    y_test = label_binarize(true_labels, classes=class_labels)\n",
    "    if n_classes == 2:\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            prob = clf.predict_proba(features)\n",
    "            y_score = prob[:, prob.shape[1]-1] \n",
    "        elif hasattr(clf, 'decision_function'):\n",
    "            prob = clf.decision_function(features)\n",
    "            y_score = prob[:, prob.shape[1]-1]\n",
    "        else:\n",
    "            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n",
    "        \n",
    "        fpr, tpr, _ = roc_curve(y_test, y_score)      \n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label='ROC curve (area = {0:0.2f})'\n",
    "                                 ''.format(roc_auc),\n",
    "                 linewidth=2.5)\n",
    "        \n",
    "    elif n_classes > 2:\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            y_score = clf.predict_proba(features)\n",
    "        elif hasattr(clf, 'decision_function'):\n",
    "            y_score = clf.decision_function(features)\n",
    "        else:\n",
    "            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        ## Compute micro-average ROC curve and ROC area\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "        ## Compute macro-average ROC curve and ROC area\n",
    "        # First aggregate all false positive rates\n",
    "        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "        # Then interpolate all ROC curves at this points\n",
    "        mean_tpr = np.zeros_like(all_fpr)\n",
    "        for i in range(n_classes):\n",
    "            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "        # Finally average it and compute AUC\n",
    "        mean_tpr /= n_classes\n",
    "        fpr[\"macro\"] = all_fpr\n",
    "        tpr[\"macro\"] = mean_tpr\n",
    "        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        ## Plot ROC curves\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                 label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"micro\"]), linewidth=3)\n",
    "\n",
    "        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                 label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"macro\"]), linewidth=3)\n",
    "\n",
    "        for i, label in enumerate(class_labels):\n",
    "            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                                           ''.format(label, roc_auc[i]), \n",
    "                     linewidth=2, linestyle=':')\n",
    "    else:\n",
    "        raise ValueError('Number of classes should be atleast 2 or more')\n",
    "        \n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-56f08012e65d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Logistic Regression model on TF-IDF features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m lr_tfidf_predictions = train_predict_model(classifier=lr, train_features=tv_text_features, train_labels=offen_train,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                            test_features=tv_test_features, test_labels=offen_test)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m display_model_performance_metrics(true_labels=offen_test, predicted_labels=lr_tfidf_predictions,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lr' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance metrics:\n",
      "------------------------------\n",
      "Accuracy: 0.9528\n",
      "Precision: 0.9506\n",
      "Recall: 0.9528\n",
      "F1 Score: 0.949\n",
      "\n",
      "Model Classification report:\n",
      "------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97     28657\n",
      "           1       0.89      0.62      0.73      3258\n",
      "\n",
      "    accuracy                           0.95     31915\n",
      "   macro avg       0.92      0.80      0.85     31915\n",
      "weighted avg       0.95      0.95      0.95     31915\n",
      "\n",
      "\n",
      "Prediction Confusion Matrix:\n",
      "------------------------------\n",
      "          Predicted:      \n",
      "                   0     1\n",
      "Actual: 0      28402   255\n",
      "        1       1251  2007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.6/site-packages/ipykernel_launcher.py:57: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "/home/david/.local/lib/python3.6/site-packages/ipykernel_launcher.py:59: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n"
     ]
    }
   ],
   "source": [
    "lr_tfidf_predictions = train_predict_model(classifier=lr, train_features=cv_text_features, train_labels=offen_train,\n",
    "                                           test_features=cv_test_features, test_labels=offen_test)\n",
    "\n",
    "display_model_performance_metrics(true_labels=offen_test, predicted_labels=lr_tfidf_predictions,\n",
    "                                      classes=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
