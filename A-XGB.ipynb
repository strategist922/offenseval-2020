{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data in 0.10s\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "import csv\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "olid_data = 'data/OLIDv1.0/olid-training-v1.0.tsv'\n",
    "\n",
    "np.random.seed(1234) # help reproducibility\n",
    "\n",
    "# y == 0 if not offensive\n",
    "# y == 1 if offensive\n",
    "start = time()\n",
    "with open(olid_data) as f:\n",
    "    raw = csv.reader(f, delimiter='\\t')\n",
    "    x_raw = []\n",
    "    y = []\n",
    "    for r in raw:\n",
    "        x_raw.append(r[1])\n",
    "        y.append(0 if r[2] == 'NOT' else 1)\n",
    "    x_raw = x_raw[1:]\n",
    "    y = np.array(y[1:])\n",
    "    bad_words = [row[:-1] for row in f.readlines()[1:]]\n",
    "print('Loaded data in %.2fs' % (time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_together(x, y):\n",
    "    # Shuffle x and y together\n",
    "    state = np.random.get_state()\n",
    "    i = np.arange(x.shape[0])\n",
    "    np.random.shuffle(i)\n",
    "    np.random.set_state(state)\n",
    "    k = np.arange(y.shape[0])\n",
    "    np.random.shuffle(k)\n",
    "    return x[i, :], y[k] # shuffling a sparse matrix is weird\n",
    "\n",
    "def report(y, y_hat, metrics=['accuracy', 'precision', 'recall', 'f1', 'auc']):\n",
    "    results = []\n",
    "    if 'accuracy' in metrics or 'acc' in metrics:\n",
    "        results.append(skmetrics.accuracy_score(y, y_hat))\n",
    "    if 'precision' in metrics:\n",
    "        results.append(skmetrics.precision_score(y, y_hat))\n",
    "    if 'recall' in metrics:\n",
    "        results.append(skmetrics.recall_score(y, y_hat))\n",
    "    if 'f1' in metrics:\n",
    "        results.append(skmetrics.f1_score(y, y_hat, average='weighted'))\n",
    "    if 'auc' in metrics:\n",
    "        results.append(skmetrics.roc_auc_score(y, y_hat))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tokenizer = TweetTokenizer()  \n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer.tokenize, \n",
    "                             strip_accents='unicode', \n",
    "                             lowercase=True,\n",
    "                             sublinear_tf=True,\n",
    "                             min_df=9,\n",
    "                             stop_words='english'\n",
    "                            )\n",
    "x = vectorizer.fit_transform(x_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "md=3, lr=0.050000, n=500\n",
      "acc     p       r       f1      auc     time    \n",
      "0.7424  0.8070  0.3094  0.7025  0.6359  12.16s\n",
      "0.7523  0.8528  0.3138  0.7120  0.6433  12.29s\n",
      "0.7545  0.8599  0.3755  0.7231  0.6707  12.11s\n",
      "0.7689  0.8133  0.3050  0.7301  0.6373  15.03s\n",
      "0.7530  0.8043  0.3371  0.7180  0.6482  14.17s\n",
      "0.7523  0.8343  0.3197  0.7136  0.6440  14.95s\n",
      "0.7674  0.8614  0.3341  0.7309  0.6542  12.70s\n",
      "0.7379  0.8261  0.2942  0.6944  0.6311  13.76s\n",
      "0.7477  0.8084  0.3089  0.7080  0.6364  14.45s\n",
      "0.7553  0.8222  0.3364  0.7197  0.6501  13.37s\n",
      "average:\n",
      "0.7532  0.8290  0.3234  0.7152  0.6451  13.50s\n",
      "\n",
      "md=3, lr=0.100000, n=500\n",
      "acc     p       r       f1      auc     time    \n",
      "0.7492  0.7664  0.3677  0.7198  0.6554  12.61s\n",
      "0.7621  0.8107  0.3770  0.7327  0.6664  12.57s\n",
      "0.7644  0.8432  0.4198  0.7393  0.6882  13.30s\n",
      "0.7704  0.7637  0.3475  0.7395  0.6505  12.54s\n",
      "0.7636  0.8088  0.3759  0.7342  0.6659  12.95s\n",
      "0.7606  0.8069  0.3696  0.7303  0.6627  12.40s\n",
      "0.7711  0.8272  0.3692  0.7407  0.6662  12.41s\n",
      "0.7515  0.8254  0.3451  0.7168  0.6536  12.58s\n",
      "0.7568  0.7833  0.3638  0.7265  0.6571  12.33s\n",
      "0.7674  0.7920  0.4068  0.7426  0.6768  12.13s\n",
      "average:\n",
      "0.7617  0.8028  0.3742  0.7323  0.6643  12.58s\n",
      "\n",
      "md=3, lr=0.500000, n=500\n",
      "acc     p       r       f1      auc     time    \n",
      "0.7576  0.7090  0.4753  0.7430  0.6881  12.43s\n",
      "0.7560  0.7013  0.4718  0.7414  0.6854  12.52s\n",
      "0.7674  0.7804  0.4873  0.7515  0.7054  12.73s\n",
      "0.7704  0.6739  0.4650  0.7566  0.6838  12.75s\n",
      "0.7613  0.7143  0.4670  0.7458  0.6872  13.36s\n",
      "0.7568  0.7045  0.4649  0.7414  0.6837  12.04s\n",
      "0.7681  0.7065  0.4836  0.7545  0.6938  12.90s\n",
      "0.7477  0.7092  0.4425  0.7297  0.6742  12.59s\n",
      "0.7734  0.7188  0.5149  0.7621  0.7078  12.45s\n",
      "0.7500  0.6677  0.4932  0.7388  0.6855  12.71s\n",
      "average:\n",
      "0.7609  0.7086  0.4765  0.7465  0.6895  12.65s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def test(model):\n",
    "    k = 10\n",
    "    kf = KFold(n_splits=k)\n",
    "    average_acc = 0\n",
    "    print(6 * '%-8s' % ('acc', 'p', 'r', 'f1', 'auc', 'time'))\n",
    "    averages = np.array([0] * 6, dtype='float')\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        start = time()\n",
    "        # Split based on k-fold\n",
    "        x_train, x_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        x_train, y_train = shuffle_together(x_train, y_train)\n",
    "        clf = model.fit(x_train, y_train)\n",
    "        y_hat = clf.predict(x_test)\n",
    "        vals = report(y_test, y_hat) + [time() - start]\n",
    "        averages += vals\n",
    "        print('%.4f  %.4f  %.4f  %.4f  %.4f  %.2fs' % tuple(vals))\n",
    "    print('average:')\n",
    "    averages /= k\n",
    "    print('%.4f  %.4f  %.4f  %.4f  %.4f  %.2fs' % tuple(averages))\n",
    "\n",
    "# md=3, lr=0.9, n=100 => 0.7452\n",
    "# md=3, lr=,    n=500 =>\n",
    "for md in [3]:\n",
    "    for lr in [0.05, 0.1, 0.5]:\n",
    "        for n in [500]:\n",
    "            print('md=%d, lr=%f, n=%d' % (md, lr,n))\n",
    "            model = XGBClassifier(max_depth=md, learning_rate=lr, n_estimators=n)\n",
    "            test(model)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
