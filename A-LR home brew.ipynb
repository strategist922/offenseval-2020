{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "olid_data = 'data/OLIDv1.0/olid-training-v1.0.tsv'\n",
    "kaggle_data = 'data/jigsaw/train.csv'\n",
    "bad_words_location = 'data/trimmed-bad-words.txt'\n",
    "\n",
    "np.random.seed(1234) # help reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y == 0 if not offensive\n",
    "# y == 1 if offensive\n",
    "start = time()\n",
    "with open(olid_data) as f:\n",
    "    raw = [x.strip().split('\\t') for x in f.readlines()[1:]]\n",
    "    x_raw = [tokenizer.tokenize(r[1]) for r in raw]\n",
    "    y = np.array([0 if r[2] == 'NOT' else 1 for r in raw])\n",
    "    \n",
    "with open(kaggle_data) as f:  \n",
    "    raw = csv.reader(f, delimiter=',')\n",
    "    kaggle_x_raw = []\n",
    "    kaggle_y = []\n",
    "    for r in raw:\n",
    "        kaggle_x_raw.append(tokenizer.tokenize(r[1]))\n",
    "        kaggle_y.append(0 if all(x == '0' for x in r[2:]) else 1)\n",
    "    kaggle_x_raw = kaggle_x_raw[1:]\n",
    "    kaggle_y = np.array(kaggle_y[1:])\n",
    "        \n",
    "with open(bad_words_location) as f:\n",
    "    bad_words = [row[:-1] for row in f.readlines()[1:]]\n",
    "\n",
    "print('Loaded data in %.2fs' % (time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocab\n",
    "start = time()\n",
    "id2word = {}\n",
    "word2id = {}\n",
    "i = 0 # index of unique word\n",
    "for tweet in x_raw: # + kaggle_x_raw:\n",
    "    for word in tweet:\n",
    "        if word not in word2id:\n",
    "            word2id[word] = i\n",
    "            id2word[i] = word\n",
    "            i += 1\n",
    "print('Vocabulary built in %.2fs' % (time() - start))\n",
    "\n",
    "# Build x bag of words\n",
    "start = time()\n",
    "data = []\n",
    "rows = []\n",
    "cols = []\n",
    "for i in range(len(x_raw)):\n",
    "    counts = {}\n",
    "    for word in x_raw[i]:\n",
    "        if word2id[word] not in counts:\n",
    "            counts[word2id[word]] = 1\n",
    "        else:\n",
    "            counts[word2id[word]] += 1\n",
    "            \n",
    "    for word_id, freq in counts.items():\n",
    "        data.append(freq)\n",
    "        rows.append(i)\n",
    "        cols.append(word_id)\n",
    "        \n",
    "    # bias\n",
    "    data.append(1)\n",
    "    rows.append(i)\n",
    "    cols.append(len(word2id))\n",
    "\n",
    "x = csr_matrix((data, (rows, cols)))\n",
    "# normalize(x, norm='l1', axis=1, copy=False) # normalize by document (TF)\n",
    "print('x BOW built in %.2fs' % (time() - start))\n",
    "\n",
    "kagged = '''\n",
    "# Build kaggle bag of words\n",
    "start = time()\n",
    "data = []\n",
    "rows = []\n",
    "cols = []\n",
    "for i in range(len(kaggle_x_raw)):\n",
    "    counts = {}\n",
    "    for word in kaggle_x_raw[i]:\n",
    "        if word2id[word] not in counts:\n",
    "            counts[word2id[word]] = 1\n",
    "        else:\n",
    "            counts[word2id[word]] += 1\n",
    "            \n",
    "    for word_id, freq in counts.items():\n",
    "        data.append(freq)\n",
    "        rows.append(i)\n",
    "        cols.append(word_id)\n",
    "        \n",
    "    # bias\n",
    "    data.append(1)\n",
    "    rows.append(i)\n",
    "    cols.append(len(word2id))\n",
    "\n",
    "kaggle_x = csr_matrix((data, (rows, cols)))\n",
    "# normalize(kaggle_x, norm='l1', axis=1, copy=False) # normalize by document (TF)\n",
    "print('kaggle_x BOW built in %.2fs' % (time() - start))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_together(x, y):\n",
    "    # Shuffle x and y together\n",
    "    state = np.random.get_state()\n",
    "    i = np.arange(x.shape[0])\n",
    "    np.random.shuffle(i)\n",
    "    np.random.set_state(state)\n",
    "    k = np.arange(y.shape[0])\n",
    "    np.random.shuffle(k)\n",
    "    return x[i, :], y[k] # shuffling a sparse matrix is weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = np.zeros(x.shape)\n",
    "    s[x > 0] = 1. / (1. + np.exp(-x[x > 0])) # avoid overflow\n",
    "    s[x <= 0] = np.exp(x[x <= 0]) / (np.exp(x[x <= 0]) + 1) # avoid underflow\n",
    "    return s\n",
    "\n",
    "def cost(w, x, y, L):\n",
    "    \"\"\" The cost function for logistic regression with L2 regularization\"\"\"\n",
    "    h = sigmoid(x @ w)\n",
    "    ridge = L * np.sum(w**2)\n",
    "    cost = np.sum(-y * np.log(h) - (1 - y) * np.log(1 - h) + ridge) / y.size\n",
    "    return cost\n",
    "\n",
    "def gradient_update(w, x, y, L):\n",
    "    \"\"\" The gradient update for logistic regression with L2 regularization\"\"\"\n",
    "    h = sigmoid(x @ w)\n",
    "    g = (h - y) @ x + (2 * L * np.sum(w))\n",
    "    g = g / x.shape[0] \n",
    "    return g\n",
    "\n",
    "def gradient_descent(w, x, y, x_val, y_val, alpha, L, iterations, print_iterations):\n",
    "    \"\"\" Batch gradient descent algorithm with early stopping\"\"\"\n",
    "    best_w = w.copy()\n",
    "    costs = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    best_acc = 0\n",
    "    alpha *= x.shape[0]\n",
    "    for i in range(iterations):\n",
    "        costs.append(cost(w, x, y, L))\n",
    "        _, y_hat = predict(w, x)\n",
    "        train_accs.append(y[np.where(y_hat == y)].size / y.size)\n",
    "        _, y_hat = predict(w, x_val)\n",
    "        val_accs.append(y_val[np.where(y_hat == y_val)].size / y_val.size)\n",
    "        \n",
    "        if i % print_iterations == 0:\n",
    "            print('%d) cost: %f' % (i, costs[-1]))\n",
    "        if val_accs[-1] > best_acc:\n",
    "            best_w = w.copy()\n",
    "            best_acc = val_accs[-1]\n",
    "        w -= alpha * gradient_update(w, x, y, L)   \n",
    "    return best_w, costs, train_accs, val_accs\n",
    "\n",
    "def predict(w, x):\n",
    "    \"\"\" Predict whether the label is 0 or 1 using learned logistic regression parameters \"\"\"\n",
    "    h = x @ w\n",
    "    probabilities = sigmoid(h)\n",
    "    predicted = 1 * (h > 0) # converts truth values to 1 or 0\n",
    "    return probabilities, 1 * predicted\n",
    "\n",
    "def report(y, y_hat, metrics=['accuracy', 'precision', 'recall', 'f1', 'auc']):\n",
    "    results = []\n",
    "    if 'accuracy' in metrics or 'acc' in metrics:\n",
    "        results.append(skmetrics.accuracy_score(y, y_hat))\n",
    "    if 'precision' in metrics:\n",
    "        results.append(skmetrics.precision_score(y, y_hat))\n",
    "    if 'recall' in metrics:\n",
    "        results.append(skmetrics.recall_score(y, y_hat))\n",
    "    if 'f1' in metrics:\n",
    "        results.append(skmetrics.f1_score(y, y_hat))\n",
    "    if 'auc' in metrics:\n",
    "        results.append(skmetrics.roc_auc_score(y, y_hat))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cross_validate(x, y, k, alpha, L):\n",
    "    # k-fold cross validation\n",
    "    chunk_size = x.shape[0] // k\n",
    "    # alpha = 0.0001\n",
    "    # L = 0.008\n",
    "    iterations = 10000\n",
    "    print_iterations = iterations // 5\n",
    "    metrics = [[], [], [], [], [], [], [], []] # acc, precision, recall, f1, auc, costs, train_accs, val_accs\n",
    "    for i in range(k):\n",
    "        start = time()\n",
    "        initial_w = np.random.rand(x.shape[1]) - 0.5\n",
    "        x_train = vstack((x[:i * chunk_size], x[(i + 1) * chunk_size:]))#, kaggle_x[:10000]))\n",
    "        y_train = np.concatenate((y[:i * chunk_size], y[(i + 1) * chunk_size:]))#, kaggle_y[:10000]))\n",
    "        x_val = x[i * chunk_size:(i + 1) * chunk_size]\n",
    "        y_val = y[i * chunk_size:(i + 1) * chunk_size]\n",
    "        x_train, y_train = shuffle_together(x_train, y_train)\n",
    "        w, costs, train_accs, val_accs = gradient_descent(initial_w.copy(), x_train, y_train, x_val, y_val, alpha, L, iterations, print_iterations)\n",
    "        plt.figure()\n",
    "        plt.yticks([p / 100 for p in range(0, 100, 5)])\n",
    "        plt.plot([e for e in range(iterations)], train_accs, c='b')\n",
    "        plt.plot([e for e in range(iterations)], val_accs, c='g')\n",
    "        plt.ylim((0.5, 0.9))\n",
    "        plt.show()\n",
    "        _, labels = predict(w, x_train)\n",
    "        [a] = report(y_train, labels, metrics=['acc'])\n",
    "        print('train acc: %.4f%%' % (a * 100))\n",
    "        _, labels = predict(w, x_val)\n",
    "        a, p, r, f, auc = report(y_val, labels)\n",
    "        metrics[0].append(a)\n",
    "        metrics[1].append(p)\n",
    "        metrics[2].append(r)\n",
    "        metrics[3].append(f)\n",
    "        metrics[4].append(auc)\n",
    "        metrics[5].append(costs)\n",
    "        metrics[6].append(train_accs)\n",
    "        metrics[7].append(val_accs)\n",
    "        print('val acc:   %.4f%%' % (a * 100))\n",
    "        print('took %.2fs' % (time() - start))\n",
    "    #plt.figure()\n",
    "    #plt.yticks([p / 100 for p in range(0, 100, 5)])\n",
    "    #plt.plot([e for e in range(iterations)], np.average(metrics[6], axis=0), c='b')\n",
    "    #plt.plot([e for e in range(iterations)], np.average(metrics[7], axis=0), c='g')\n",
    "    #plt.ylim((0.5, 0.9))\n",
    "    #plt.show()\n",
    "    print('avg val accuracy:   %.4f' % np.average(metrics[0]))\n",
    "    print('avg val precision:  %.4f' % np.average(metrics[1]))\n",
    "    print('avg val recall:     %.4f' % np.average(metrics[2]))\n",
    "    print('avg val f1:         %.4f' % np.average(metrics[3]))\n",
    "    print('avg val auc:        %.4f' % np.average(metrics[4]))\n",
    "    return metrics\n",
    "for L in [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]:\n",
    "    metrics = cross_validate(x, y, 5, 0.00002, L)\n",
    "    print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### predicted = []\n",
    "for i in range(len(x_raw)):\n",
    "    offensive = False\n",
    "    for word in x_raw[i].split():\n",
    "        if word in bad_words:\n",
    "            offensive = True\n",
    "            break\n",
    "    if offensive:\n",
    "        predicted.append(1)\n",
    "    else:\n",
    "        predicted.append(0)\n",
    "rule_based_labels = np.array(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "off = 100 * (np.sum(y) / y.shape[0])\n",
    "not_off = 100 * (1 - (np.sum(y) / y.shape[0]))\n",
    "print('Offensive\\tNot offensive')\n",
    "print('%.2f%%\\t\\t%.2f%%' % (off, not_off))\n",
    "labels = np.zeros(y.shape)\n",
    "a, p, r, f = report(y, labels)\n",
    "print('accuracy:  %.4f' % a)\n",
    "print('precision: %.4f' % p)\n",
    "print('recall:    %.4f' % r)\n",
    "print('f1:        %.4f' % f)\n",
    "\n",
    "a, p, r, f = report(y, rule_based_labels)\n",
    "print('\\nOffensive if tweet contains a bad word:')\n",
    "print('accuracy:  %.4f' % a)\n",
    "print('precision: %.4f' % p)\n",
    "print('recall:    %.4f' % r)\n",
    "print('f1:        %.4f' % f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "off_marks = 0\n",
    "unoff_marks = 0\n",
    "for tweet, label in zip(x_raw, y):\n",
    "    for token in tweet:\n",
    "        if token == '!':\n",
    "            if label == 1:\n",
    "                off_marks += 1\n",
    "            else:\n",
    "                unoff_marks += 1\n",
    "off = np.sum(y)\n",
    "unoff = y.size - off\n",
    "print('average ! per offensive tweet: %.4f' % (off_marks / off))\n",
    "print('average ! per unoffensive tweet: %.4f' % (unoff_marks / unoff))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
