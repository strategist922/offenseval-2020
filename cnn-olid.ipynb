{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as skmetrics\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "\n",
    "np.random.seed(1234) # help reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, num_classes, max_tweet_length, embeddings=None):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        if embeddings is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(embeddings)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=2)\n",
    "#         self.conv3 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=3)\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=max_tweet_length+1, padding=(max_tweet_length+1) // 2)\n",
    "        self.g = nn.ReLU()\n",
    "        self.linear = nn.Linear(in_features=num_filters * 2, out_features=num_classes)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        emb = self.embedding(x).T\n",
    "        emb = emb.unsqueeze(0)\n",
    "        \n",
    "        c1 = self.conv1(emb)\n",
    "        c2 = self.conv2(emb)\n",
    "#         c3 = self.conv3(emb)\n",
    "        \n",
    "        c1_pooled = self.max_pool(c1)\n",
    "        c2_pooled = self.max_pool(c2)\n",
    "#         c3_pooled = self.max_pool(c3)\n",
    "        \n",
    "        h1 = self.g(c1_pooled)\n",
    "        h2 = self.g(c2_pooled)\n",
    "#         h3 = self.g(c3_pooled)\n",
    "        \n",
    "        try:\n",
    "#             all_out = torch.cat((h1, h2, h3), 1).squeeze()\n",
    "            all_out = torch.cat((h1, h2), 1).squeeze()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(h1.shape)\n",
    "            print(h2.shape)\n",
    "#             print(h3.shape)\n",
    "            sys.exit()\n",
    "        \n",
    "        y_hat = self.linear(all_out)\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    def train(self, X, Y, iterations, learning_rate):\n",
    "        print('beginning training...')\n",
    "        num_samples = len(Y)\n",
    "        \n",
    "        optimizer = optim.Adam(params=self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        for e in range(iterations):\n",
    "            # randomize order of samples at each epoch\n",
    "            rand_idx = np.random.permutation(num_samples)\n",
    "\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            for i in rand_idx:\n",
    "                x = X[i]\n",
    "                y = Y[i].long().unsqueeze(0)\n",
    "                \n",
    "                self.zero_grad()\n",
    "                y_hat = self(x).unsqueeze(0)\n",
    "                \n",
    "                loss = self.loss.forward(y_hat, y)\n",
    "                total_loss += loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "#                 if count % 1000 == 0:\n",
    "#                     print(f'loss as of sample {count}: {total_loss}')\n",
    "                    \n",
    "#                 count += 1\n",
    "\n",
    "            print(f'total loss on epoch {e}: {total_loss}')\n",
    "        print('done training!')\n",
    "        return\n",
    "    \n",
    "    def eval(self, X, Y, idx=None):\n",
    "        print('evaluating...')\n",
    "        Y_hat = torch.zeros(len(Y))\n",
    "        \n",
    "        if idx is not None:\n",
    "            fp = open('predictions/cnn-validation.csv', 'w')\n",
    "            fp.write('id,prediction\\n')\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            y_prob = self(X[i])\n",
    "            pred = torch.argmax(y_prob)\n",
    "            Y_hat[i] = pred\n",
    "            if idx is not None:\n",
    "                fp.write(f'{idx[i]},{pred}\\n')\n",
    "                \n",
    "        \n",
    "        if idx is not None:\n",
    "            fp.close()\n",
    "        \n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        metrics['acc'] = skmetrics.accuracy_score(Y, Y_hat)\n",
    "        metrics['f1'] = skmetrics.f1_score(Y, Y_hat)\n",
    "        metrics['prec'] = skmetrics.precision_score(Y, Y_hat)\n",
    "        metrics['rec'] = skmetrics.recall_score(Y, Y_hat)\n",
    "        \n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(word2id, embedding_dim):\n",
    "    print('loading word embeddings...')\n",
    "    embedding_file = 'data/glove.twitter.27B/glove.twitter.27B.%dd.txt' % embedding_dim\n",
    "    \n",
    "    embeddings = torch.empty(len(word2id), embedding_dim)\n",
    "    glove_embeddings = {}\n",
    "    \n",
    "    with open(embedding_file) as fp:\n",
    "        for line in fp.readlines():\n",
    "            arr = line.split()\n",
    "            glove_embeddings[arr[0]] = np.array(arr[1:embedding_dim+1]).astype(float)\n",
    "        \n",
    "    for w, idx in word2id.items():\n",
    "        if w in glove_embeddings:\n",
    "            embeddings[idx] = torch.LongTensor(glove_embeddings[w])\n",
    "        else:\n",
    "            embeddings[idx] = torch.randn(embedding_dim)\n",
    "            \n",
    "    return embeddings\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocabulary...\n"
     ]
    }
   ],
   "source": [
    "# **** FOR TRAINING AND TESTING ON OLID TRAINING SET ****\n",
    "\n",
    "# extract data from file and format accordingly\n",
    "data = pd.read_csv('data/OLIDv1.0/olid-training-v1.0.tsv', '\\t')\n",
    "X_raw = data['tweet'].values\n",
    "tknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "# build vocab\n",
    "print('building vocabulary...')\n",
    "vocab = Counter()\n",
    "for tweet in data['tweet']:\n",
    "    vocab.update(w for w in tknzr.tokenize(tweet))\n",
    "    \n",
    "vocab = sorted(vocab, key=vocab.get, reverse=True)\n",
    "\n",
    "word2id = {w:i for i,w in enumerate(vocab)}\n",
    "id2word = {i:w for i,w in enumerate(vocab)}\n",
    "\n",
    "def convert_to_idx(tweet):\n",
    "    return [word2id[w] for w in tknzr.tokenize(tweet)]\n",
    "\n",
    "data['tweet_tokenized'] = data['tweet'].apply(convert_to_idx)\n",
    "data['length'] = data['tweet_tokenized'].apply(len)\n",
    "max_tweet_len = int(data['length'].max())\n",
    "\n",
    "data_without_short_tweets = data.loc[data['length'] >= 3]\n",
    "\n",
    "X = [x for x in data_without_short_tweets['tweet_tokenized']]\n",
    "\n",
    "\n",
    "Y_raw = data_without_short_tweets['subtask_a'].values\n",
    "Y = np.zeros(len(Y_raw))\n",
    "Y[np.where(Y_raw == 'OFF')] = 1.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2)\n",
    "\n",
    "# convert Xs to tensors\n",
    "X_train = [torch.LongTensor(x) for x in X_train]\n",
    "X_test = [torch.LongTensor(x) for x in X_test]\n",
    "Y_train = torch.Tensor(Y_train)\n",
    "Y_test = torch.Tensor(Y_test)\n",
    "\n",
    "# this prevents printing to predictions/cnn-validation.csv\n",
    "idx_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocabulary...\n",
      "converting tweets to IDs...\n",
      "removing short tweets...\n",
      "done prepping data\n"
     ]
    }
   ],
   "source": [
    "# **** FOR TRAINING ON OLID TRAING AND TESTING ON OLID VALIDATION\n",
    "\n",
    "training_df = pd.read_csv('data/OLIDv1.0/olid-training-v1.0.tsv', '\\t')\n",
    "\n",
    "test_df = pd.read_csv('data/OLIDv1.0/testset-levela.tsv', '\\t')\n",
    "Y_test_df = pd.read_csv('data/OLIDv1.0/labels-levela.csv', names=['id', 'label'])\n",
    "\n",
    "test_df['label'] = Y_test_df['label']\n",
    "\n",
    "\n",
    "tknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "# build vocab over both training and test sets\n",
    "print('building vocabulary...')\n",
    "vocab = Counter()\n",
    "for tweet in training_df['tweet'].append(test_df['tweet']):\n",
    "    vocab.update(w for w in tknzr.tokenize(tweet))\n",
    "    \n",
    "vocab = sorted(vocab, key=vocab.get, reverse=True)\n",
    "\n",
    "word2id = {w:i for i,w in enumerate(vocab)}\n",
    "id2word = {i:w for i,w in enumerate(vocab)}\n",
    "\n",
    "def convert_to_idx(tweet):\n",
    "    return [word2id[w] for w in tknzr.tokenize(tweet)]\n",
    "\n",
    "print('converting tweets to IDs...')\n",
    "training_df['tweet_tokenized'] = training_df['tweet'].apply(convert_to_idx)\n",
    "training_df['length'] = training_df['tweet_tokenized'].apply(len)\n",
    "\n",
    "test_df['tweet_tokenized'] = test_df['tweet'].apply(convert_to_idx)\n",
    "test_df['length'] = test_df['tweet_tokenized'].apply(len)\n",
    "\n",
    "print('removing short tweets...')\n",
    "max_tweet_len = int((training_df['length'].append(test_df['length'])).max())\n",
    "\n",
    "train_without_short_tweets = training_df.loc[training_df['length'] >= 3]\n",
    "test_without_short_tweets = test_df.loc[test_df['length'] >= 3]\n",
    "\n",
    "X_train = [x for x in train_without_short_tweets['tweet_tokenized']]\n",
    "X_test = [x for x in test_without_short_tweets['tweet_tokenized']]\n",
    "\n",
    "Y_train_raw = train_without_short_tweets['subtask_a'].values\n",
    "Y_train = np.zeros(len(Y_train_raw))\n",
    "Y_train[np.where(Y_train_raw == 'OFF')] = 1.\n",
    "\n",
    "Y_test_raw = test_without_short_tweets['label'].values\n",
    "Y_test = np.zeros(len(Y_test_raw))\n",
    "Y_test[np.where(Y_test_raw == 'OFF')] = 1.\n",
    "\n",
    "# this means we will print to predictions/cnn-validation.csv\n",
    "idx_test = test_without_short_tweets['id'].values\n",
    "\n",
    "X_train = [torch.LongTensor(x) for x in X_train]\n",
    "X_test = [torch.LongTensor(x) for x in X_test]\n",
    "Y_train = torch.Tensor(Y_train)\n",
    "Y_test = torch.Tensor(Y_test)\n",
    "print('done prepping data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yooooo\n",
      "beginning training...\n",
      "total loss on epoch 0: 10224.8271484375\n",
      "total loss on epoch 1: 9280.5302734375\n",
      "total loss on epoch 2: 8348.5615234375\n",
      "done training!\n",
      "evaluating...\n",
      "Metrics:\n",
      "accuracy: 0.79463243873979\n",
      "f1: 0.5294117647058822\n",
      "precision: 0.7333333333333333\n",
      "recall: 0.41422594142259417\n"
     ]
    }
   ],
   "source": [
    "# hyper params\n",
    "EPOCHS = 3\n",
    "LR = .01\n",
    "EMBEDDING_DIM = 10\n",
    "NUM_FILTERS = 200\n",
    "\n",
    "\n",
    "# embeddings = get_embeddings(word2id, EMBEDDING_DIM)\n",
    "model = CNN(vocab_size=len(vocab), embedding_dim=EMBEDDING_DIM, num_filters=NUM_FILTERS, num_classes=2, max_tweet_length=max_tweet_len)\n",
    "model.train(X_train, Y_train, EPOCHS, LR)\n",
    "metrics = model.eval(X_test, Y_test, idx_test)\n",
    "\n",
    "acc = metrics['acc']\n",
    "f1 = metrics['f1']\n",
    "prec = metrics['prec']\n",
    "rec = metrics['rec']\n",
    "\n",
    "print('Metrics:')\n",
    "print(f'accuracy: {acc}')\n",
    "print(f'f1: {f1}')\n",
    "print(f'precision: {prec}')\n",
    "print(f'recall: {rec}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "#### self trained embeddings\n",
    "\n",
    "* three sizes of filter (1, 2, 3)\n",
    "* 0.01 learning rate\n",
    "* 3 epochs\n",
    "\n",
    "  * accuracy: 0.6874760628111835\n",
    "  * f1: 0.4708171206225681\n",
    "  * precision: 0.5208034433285509\n",
    "  * recall: 0.42958579881656805\n",
    " \n",
    "* three sizes of filter (1, 2, 3)\n",
    "* 0.01 learning rate\n",
    "* 4 epochs\n",
    "\n",
    "  * accuracy: 0.7338184603600153\n",
    "  * f1: 0.42514474772539296\n",
    "  * precision: 0.6710182767624021\n",
    "  * recall: 0.31113801452784506\n",
    "  \n",
    "* three sizes of filter (1, 2, 3)\n",
    "* 0.01 learning rate\n",
    "* 5 epochs\n",
    "\n",
    "  * accuracy: 0.6974339333588664\n",
    "  * f1: 0.5135467980295566\n",
    "  * precision: 0.5567423230974633\n",
    "  * recall: 0.4765714285714286\n",
    "  \n",
    "* three sizes of filter (1, 2, 3)\n",
    "* 0.001 learning rate\n",
    "* 5 epochs\n",
    "\n",
    "  * accuracy: 0.7123707391803906\n",
    "  * f1: 0.5406727828746177\n",
    "  * precision: 0.5755208333333334\n",
    "  * recall: 0.5098039215686274\n",
    "  \n",
    "* two sizes of filter (1, 2)\n",
    "* 0.01 learning rate\n",
    "* 4 epochs\n",
    "\n",
    "  * accuracy: 0.7280735350440444\n",
    "  * f1: 0.5069444444444444\n",
    "  * precision: 0.6529516994633273\n",
    "  * recall: 0.41430192962542567\n",
    "  \n",
    "* two sizes of filter (1, 2)\n",
    "* 0.001 learning rate\n",
    "* 5 epochs\n",
    "\n",
    "  * accuracy: 0.7261585599387208\n",
    "  * f1: 0.3986543313708999\n",
    "  * precision: 0.7337461300309598\n",
    "  * recall: 0.27367205542725176\n",
    "  \n",
    "* two sizes of filter (1, 2)\n",
    "* 0.01 learning rate\n",
    "* 3 epochs\n",
    "\n",
    "  * accuracy: 0.7342014553810801\n",
    "  * f1: 0.5475880052151239\n",
    "  * precision: 0.6451612903225806\n",
    "  * recall: 0.47565118912797283\n",
    "  \n",
    "#### pre-trained embeddings\n",
    "\n",
    "* two sizes of filter (1, 2)\n",
    "* 0.01 learning rate\n",
    "* 3 epochs\n",
    "\n",
    "  * accuracy: 0.665645346610494\n",
    "  * f1: 0.3361216730038023\n",
    "  * precision: 0.5224586288416075\n",
    "  * recall: 0.24775784753363228"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
