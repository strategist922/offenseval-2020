{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as skmetrics\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "\n",
    "np.random.seed(1234) # help reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_filters, num_classes, max_tweet_length):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=2)\n",
    "#         self.conv3 = nn.Conv1d(in_channels=embedding_dim, out_channels=num_filters, kernel_size=3)\n",
    "        self.max_pool = nn.MaxPool1d(kernel_size=max_tweet_length+1, padding=(max_tweet_length+1) // 2)\n",
    "        self.g = nn.ReLU()\n",
    "        self.linear = nn.Linear(in_features=num_filters * 2, out_features=num_classes)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        emb = self.embedding(x).T\n",
    "        emb = emb.unsqueeze(0)\n",
    "        \n",
    "        c1 = self.conv1(emb)\n",
    "        c2 = self.conv2(emb)\n",
    "#         c3 = self.conv3(emb)\n",
    "        \n",
    "        c1_pooled = self.max_pool(c1)\n",
    "        c2_pooled = self.max_pool(c2)\n",
    "#         c3_pooled = self.max_pool(c3)\n",
    "        \n",
    "        h1 = self.g(c1_pooled)\n",
    "        h2 = self.g(c2_pooled)\n",
    "#         h3 = self.g(c3_pooled)\n",
    "        \n",
    "        try:\n",
    "#             all_out = torch.cat((h1, h2, h3), 1).squeeze()\n",
    "            all_out = torch.cat((h1, h2), 1).squeeze()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(h1.shape)\n",
    "            print(h2.shape)\n",
    "#             print(h3.shape)\n",
    "            sys.exit()\n",
    "        \n",
    "        y_hat = self.linear(all_out)\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    def train(self, X, Y, iterations, learning_rate):\n",
    "        num_samples = len(Y)\n",
    "        \n",
    "        optimizer = optim.Adam(params=self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        for e in range(iterations):\n",
    "            # randomize order of samples at each epoch\n",
    "            rand_idx = np.random.permutation(num_samples)\n",
    "\n",
    "            total_loss = 0\n",
    "            count = 0\n",
    "            for i in rand_idx:\n",
    "                x = X[i]\n",
    "                y = Y[i].long().unsqueeze(0)\n",
    "                \n",
    "                self.zero_grad()\n",
    "                y_hat = self(x).unsqueeze(0)\n",
    "                \n",
    "                loss = self.loss.forward(y_hat, y)\n",
    "                total_loss += loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "#                 if count % 1000 == 0:\n",
    "#                     print(f'loss as of sample {count}: {total_loss}')\n",
    "                    \n",
    "#                 count += 1\n",
    "\n",
    "            print(f'total loss on epoch {e}: {total_loss}')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def eval(self, X, Y):\n",
    "        \n",
    "        Y_hat = torch.zeros(len(Y))\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            y_prob = self(X[i])\n",
    "            Y_hat[i] = torch.argmax(y_prob)\n",
    "            \n",
    "        metrics = {}\n",
    "        \n",
    "        metrics['acc'] = skmetrics.accuracy_score(Y, Y_hat)\n",
    "        metrics['f1'] = skmetrics.f1_score(Y, Y_hat)\n",
    "        metrics['prec'] = skmetrics.precision_score(Y, Y_hat)\n",
    "        metrics['rec'] = skmetrics.recall_score(Y, Y_hat)\n",
    "        \n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss on epoch 0: 8469.501953125\n",
      "total loss on epoch 1: 7654.54052734375\n",
      "total loss on epoch 2: 6906.919921875\n",
      "Metrics:\n",
      "accuracy: 0.7342014553810801\n",
      "f1: 0.5475880052151239\n",
      "precision: 0.6451612903225806\n",
      "recall: 0.47565118912797283\n"
     ]
    }
   ],
   "source": [
    "# hyper params\n",
    "EPOCHS = 3\n",
    "LR = .01\n",
    "EMBEDDING_DIM = 10\n",
    "NUM_FILTERS = 200\n",
    "\n",
    "\n",
    "\n",
    "# extract data from file and format accordingly\n",
    "data = pd.read_csv('data/OLIDv1.0/olid-training-v1.0.tsv', '\\t')\n",
    "X_raw = data['tweet'].values\n",
    "tknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "# build vocab\n",
    "vocab = Counter()\n",
    "for tweet in data['tweet']:\n",
    "    vocab.update(w for w in tknzr.tokenize(tweet))\n",
    "    \n",
    "vocab = sorted(vocab, key=vocab.get, reverse=True)\n",
    "\n",
    "word2id = {w:i for i,w in enumerate(vocab)}\n",
    "id2word = {i:w for i,w in enumerate(vocab)}\n",
    "\n",
    "def convert_to_idx(tweet):\n",
    "    return [word2id[w] for w in tknzr.tokenize(tweet)]\n",
    "\n",
    "data['tweet_tokenized'] = data['tweet'].apply(convert_to_idx)\n",
    "data['length'] = data['tweet_tokenized'].apply(len)\n",
    "max_tweet_len = int(data['length'].max())\n",
    "\n",
    "data_without_short_tweets = data.loc[data['length'] >= 3]\n",
    "\n",
    "X = [x for x in data_without_short_tweets['tweet_tokenized']]\n",
    "\n",
    "Y_raw = data_without_short_tweets['subtask_a'].values\n",
    "Y = np.zeros(len(Y_raw))\n",
    "Y[np.where(Y_raw == 'OFF')] = 1.\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2)\n",
    "\n",
    "# convert Xs to tensors\n",
    "X_train = [torch.LongTensor(x) for x in X_train]\n",
    "X_test = [torch.LongTensor(x) for x in X_test]\n",
    "Y_train = torch.Tensor(Y_train)\n",
    "Y_test = torch.Tensor(Y_test)\n",
    "\n",
    "model = CNN(vocab_size=len(vocab), embedding_dim=EMBEDDING_DIM, num_filters=NUM_FILTERS, num_classes=2, max_tweet_length=max_tweet_len)\n",
    "model.train(X_train, Y_train, EPOCHS, LR)\n",
    "metrics = model.eval(X_test, Y_test)\n",
    "\n",
    "acc = metrics['acc']\n",
    "f1 = metrics['f1']\n",
    "prec = metrics['prec']\n",
    "rec = metrics['rec']\n",
    "\n",
    "print('Metrics:')\n",
    "print(f'accuracy: {acc}')\n",
    "print(f'f1: {f1}')\n",
    "print(f'precision: {prec}')\n",
    "print(f'recall: {rec}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "* three sizes of filter (1, 2, 3)\n",
    "* 0.01 learning rate\n",
    "* 3 epochs\n",
    "\n",
    "  * accuracy: 0.6874760628111835\n",
    "  * f1: 0.4708171206225681\n",
    "  * precision: 0.5208034433285509\n",
    "  * recall: 0.42958579881656805\n",
    " \n",
    "* three sizes of filter (1, 2, 3)\n",
    "* 0.01 learning rate\n",
    "* 4 epochs\n",
    "\n",
    "  * accuracy: 0.7338184603600153\n",
    "  * f1: 0.42514474772539296\n",
    "  * precision: 0.6710182767624021\n",
    "  * recall: 0.31113801452784506\n",
    "  \n",
    "* three sizes of filter (1, 2, 3)\n",
    "* 0.01 learning rate\n",
    "* 5 epochs\n",
    "\n",
    "  * accuracy: 0.6974339333588664\n",
    "  * f1: 0.5135467980295566\n",
    "  * precision: 0.5567423230974633\n",
    "  * recall: 0.4765714285714286\n",
    "  \n",
    "* three sizes of filter (1, 2, 3)\n",
    "* 0.001 learning rate\n",
    "* 5 epochs\n",
    "\n",
    "  * accuracy: 0.7123707391803906\n",
    "  * f1: 0.5406727828746177\n",
    "  * precision: 0.5755208333333334\n",
    "  * recall: 0.5098039215686274\n",
    "  \n",
    "* two sizes of filter (1, 2)\n",
    "* 0.01 learning rate\n",
    "* 4 epochs\n",
    "\n",
    "  * accuracy: 0.7280735350440444\n",
    "  * f1: 0.5069444444444444\n",
    "  * precision: 0.6529516994633273\n",
    "  * recall: 0.41430192962542567\n",
    "  \n",
    "* two sizes of filter (1, 2)\n",
    "* 0.001 learning rate\n",
    "* 5 epochs\n",
    "\n",
    "  * accuracy: 0.7261585599387208\n",
    "  * f1: 0.3986543313708999\n",
    "  * precision: 0.7337461300309598\n",
    "  * recall: 0.27367205542725176\n",
    "  \n",
    "* two sizes of filter (1, 2)\n",
    "* 0.01 learning rate\n",
    "* 3 epochs\n",
    "\n",
    "  * accuracy: 0.7342014553810801\n",
    "  * f1: 0.5475880052151239\n",
    "  * precision: 0.6451612903225806\n",
    "  * recall: 0.47565118912797283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
