{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available! Using device 0 (GeForce GTX 1070)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn import metrics as skmetrics\n",
    "import torch\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "embedding_dim = 25\n",
    "embedding_file = 'data/glove.twitter.27B/glove.twitter.27B.%dd.txt' % embedding_dim\n",
    "training_data = 'data/OLIDv1.0/olid-training-v1.0.tsv'\n",
    "test_data = 'data/OLIDv1.0/testset-levela.tsv'\n",
    "test_labels = 'data/OLIDv1.0/labels-levela.csv'\n",
    "hashtags = 'data/olid_segmentations.tsv'\n",
    "if torch.cuda.is_available():\n",
    "    device = 0 \n",
    "    print('CUDA available! Using device %d (%s)' % (device, torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    device = None\n",
    "    print('CUDA unavailable! Using CPU.')\n",
    "\n",
    "np.random.seed(1234) # helps reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    # https://discuss.pytorch.org/t/is-there-something-like-keras-utils-to-categorical-in-pytorch/5960\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "def report(y, y_hat, metrics=['accuracy', 'precision', 'recall', 'f1-weighted', 'f1-macro']):\n",
    "    results = []\n",
    "    metrics = metrics.copy()\n",
    "    while len(metrics) > 0:\n",
    "        m = metrics.pop(0)\n",
    "        if m == 'accuracy':\n",
    "            results.append(skmetrics.accuracy_score(y, y_hat))\n",
    "        elif m == 'precision':\n",
    "            results.append(skmetrics.precision_score(y, y_hat))\n",
    "        elif m == 'recall':\n",
    "            results.append(skmetrics.recall_score(y, y_hat))\n",
    "        elif m == 'f1-weighted':\n",
    "            results.append(skmetrics.f1_score(y, y_hat, average='weighted'))\n",
    "        elif m == 'f1-macro':\n",
    "            results.append(skmetrics.f1_score(y, y_hat, average='macro'))\n",
    "        else:\n",
    "            print('Metric unknown: %s' % m)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data in 0.05s\n",
      "Preprocessed data in 3.83s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "with open(training_data, encoding='utf-8') as f:\n",
    "    raw = csv.reader(f, delimiter='\\t')\n",
    "    x_train_raw = []\n",
    "    y_train = []\n",
    "    for r in raw:\n",
    "        x_train_raw.append(r[1])\n",
    "        y_train.append(0 if r[2] == 'NOT' else 1)\n",
    "    x_train_raw = x_train_raw[1:]\n",
    "    y_train = y_train[1:]\n",
    "    \n",
    "with open(test_data, encoding='utf-8') as f:\n",
    "    raw = csv.reader(f, delimiter='\\t')\n",
    "    test_ids = []\n",
    "    x_test_raw = []\n",
    "    for r in raw:\n",
    "        test_ids.append(r[0])\n",
    "        x_test_raw.append(r[1])\n",
    "    test_ids = [int(i) for i in test_ids[1:]]\n",
    "    x_test_raw = x_test_raw[1:]\n",
    "        \n",
    "with open(test_labels, encoding='utf-8') as f:\n",
    "    raw = csv.reader(f, delimiter=',')\n",
    "    y_test = []\n",
    "    for r in raw:\n",
    "        y_test.append(0 if r[1] == 'NOT' else 1)\n",
    "    \n",
    "segmentations = {}\n",
    "for line in open(hashtags, encoding='utf-8'):\n",
    "    terms = [x.strip().lower() for x in line.split('\\t')]\n",
    "    hashtag, segmentation = terms[0], terms[1]\n",
    "    segmentations[hashtag] = segmentation\n",
    "    \n",
    "print('Loaded data in %.2fs' % (time() - start))\n",
    "\n",
    "# Tokenize data\n",
    "tokenizer = TweetTokenizer(preserve_case=False)  \n",
    "x_train = []\n",
    "vocab = {}\n",
    "i = 2 # start from 2. 0 is unk, 1 is pad\n",
    "for tweet in x_train_raw:\n",
    "    example = []\n",
    "    for token in tokenizer.tokenize(tweet):\n",
    "        # if it's a hashtag, look up segmentaion\n",
    "        if token[0] == '#' and token[1:] in segmentations:\n",
    "            sequence = segmentations[token[1:]].split()\n",
    "        else:\n",
    "            sequence = [token]\n",
    "            \n",
    "        for word in sequence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = i\n",
    "                i += 1\n",
    "            example.append(vocab[word])\n",
    "    x_train.append(example)\n",
    "\n",
    "x_test = []\n",
    "for tweet in x_test_raw:\n",
    "    example = []\n",
    "    for token in tokenizer.tokenize(tweet):\n",
    "        # if it's a hashtag, look up segmentaion\n",
    "        if token[0] == '#' and token[1:] in segmentations:\n",
    "            sequence = segmentations[token[1:]].split()\n",
    "        else:\n",
    "            sequence = [token]\n",
    "            \n",
    "        for word in sequence:\n",
    "            if word not in vocab:\n",
    "                example.append(0)\n",
    "            else:\n",
    "                example.append(vocab[word])\n",
    "    x_test.append(example)\n",
    "        \n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Randomly shuffle\n",
    "i = np.arange(len(x_train))  \n",
    "np.random.shuffle(i)\n",
    "x_train = [torch.LongTensor(x_train[k]).to(device) for k in i]\n",
    "y_train = torch.FloatTensor(to_categorical(y_train[i], 2)).to(device)\n",
    "\n",
    "x_test = [torch.LongTensor(x_test[k]).to(device) for k in range(len(x_test))]\n",
    "y_test = torch.FloatTensor(to_categorical(y_test, 2)).to(device)\n",
    "\n",
    "# split = 0.7\n",
    "# split_index = int(len(x) * split)\n",
    "# x_train = x[:split_index]\n",
    "# y_train = y[:split_index]\n",
    "# x_val = x[split_index:]\n",
    "# y_val = y[split_index:]\n",
    "print('Preprocessed data in %.2fs' % (time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings in 15.89s\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "# On my mac, GloVe loads 25D in 30s, 50D in 100s, 100D in 630s\n",
    "start = time()\n",
    "embeddings = {}\n",
    "with open(embedding_file, encoding='utf-8') as f:\n",
    "    raw = [row.split() for row in f.readlines()]\n",
    "    for r in raw:\n",
    "        embeddings[r[0]] = np.array([float(v) for v in r[1:]])\n",
    "\n",
    "# Create embedding weight matrix that corresponds to the ids we've already set for x\n",
    "# https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "embedding_weights = np.zeros((len(vocab) + 2, embedding_dim)) # add 1 to account for unk and pad\n",
    "for word, i in vocab.items():\n",
    "    try: \n",
    "        embedding_weights[i] = embeddings[word]\n",
    "    except KeyError:\n",
    "        embedding_weights[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "embedding_weights = torch.FloatTensor(embedding_weights)\n",
    "print('Loaded embeddings in %.2fs' % (time() - start)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings=None, dim_emb=25, n_classes=2, device=0):\n",
    "        super(GRU, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_classes = n_classes\n",
    "        self.device = device\n",
    "        \n",
    "        bidirectional = False\n",
    "        gru_size = 32\n",
    "        if embeddings is None:\n",
    "            self.dim_emb = dim_emb\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.dim_emb)\n",
    "        else:\n",
    "            self.dim_emb = embeddings.shape[1]\n",
    "            self.embedding = nn.Embedding.from_pretrained(embeddings) # defaults to frozen\n",
    "        self.gru = nn.GRU(input_size=self.dim_emb,\n",
    "                          hidden_size=gru_size,\n",
    "                          num_layers=2,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=bidirectional,\n",
    "                          dropout=0.2\n",
    "                         )\n",
    "        if bidirectional:\n",
    "            self.fc1 = nn.Linear(2 * gru_size, self.n_classes)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(gru_size, self.n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, h = self.gru(x)\n",
    "        x = h.permute(1, 0, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(out[:, -1])\n",
    "        y = self.softmax(self.fc1(x))\n",
    "        return y\n",
    "    \n",
    "    def predict(self, x, one_hot=False, batch_size=32):\n",
    "        if one_hot:\n",
    "            y = torch.zeros((len(x), self.n_classes))\n",
    "        else:\n",
    "            y = torch.zeros((len(x)))\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            batch = rnn.pack_sequence(x[i:i+batch_size], enforce_sorted=False)\n",
    "            batch, _ = rnn.pad_packed_sequence(batch, batch_first=True)\n",
    "            logits = self.forward(batch)\n",
    "            preds = torch.max(logits, 1)[1]\n",
    "            if one_hot:\n",
    "                pass # broken\n",
    "                # y[i:i+batch_size, preds] = 1\n",
    "            else:\n",
    "                y[i:i+batch_size] = preds\n",
    "        return y\n",
    "\n",
    "history = []\n",
    "def train(x_train, y_train, x_val, y_val, vocab_size, batch_size, lr, epochs):\n",
    "    print('Start Training!')\n",
    "    global history\n",
    "    mlp = GRU(vocab_size, embedding_weights)\n",
    "    if torch.cuda.is_available():\n",
    "        mlp.cuda()\n",
    "    optimizer = optim.Adam(mlp.parameters(), lr=lr)\n",
    "    max_f1 = 0\n",
    "    print('epoch time   t_loss   t_acc  v_acc  t_f1m  v_f1m  v_f1w')\n",
    "    for epoch in range(epochs):\n",
    "        start = time()\n",
    "        total_loss = 0.0\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            mlp.zero_grad()\n",
    "            batch = rnn.pack_sequence(x_train[i:i+batch_size], enforce_sorted=False)\n",
    "            batch, _ = rnn.pad_packed_sequence(batch, batch_first=True)\n",
    "            probs = mlp.forward(batch)\n",
    "            NLL = torch.neg(torch.log(probs))\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "            losses = torch.bmm(NLL.view(len(y_batch), 1, 2), y_batch.view(len(y_batch), 2, 1))\n",
    "            loss = torch.sum(losses) / len(y_batch)\n",
    "            total_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        metrics = ['accuracy', 'f1-weighted', 'f1-macro']\n",
    "        t_y_hat = mlp.predict(x_train, False, 1024)\n",
    "        t_dense = torch.max(y_train, 1)[1]\n",
    "        t_acc, t_f1w, t_f1m = report(t_dense.cpu(), t_y_hat, metrics=metrics)\n",
    "        v_y_hat = mlp.predict(x_val, False, 1024)\n",
    "        v_dense = torch.max(y_val, 1)[1]\n",
    "        v_acc, v_f1w, v_f1m = report(v_dense.cpu(), v_y_hat, metrics=metrics)\n",
    "        vals = (epoch, time() - start, total_loss.item(), t_acc, v_acc, t_f1m, v_f1m, v_f1w)\n",
    "        history.append(vals)\n",
    "        print('\\r%-5d %5.2fs %08.4f %.4f %.4f %.4f %.4f %.4f' % vals, end='')\n",
    "        if v_f1m > max_f1:\n",
    "             print('\\tNew best macro f1! Saving model and predictions.')\n",
    "             torch.save(mlp.state_dict(), 'models/best-bigru.model')\n",
    "             with open('predictions/best-bigru.txt', 'w') as f:\n",
    "                for pred in v_dense:\n",
    "                    f.write('%d\\n' % pred)\n",
    "             max_f1 = v_f1m\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "epoch time   t_loss   t_acc  v_acc  t_f1m  v_f1m  v_f1w\n",
      "0     52.46s 1434.5070 0.4921 0.4988 0.4773 0.4728 0.5246\tNew best macro f1! Saving model and predictions.\n",
      "1     52.54s 1434.2445 0.5087 0.5570 0.4907 0.5097 0.5770\tNew best macro f1! Saving model and predictions.\n",
      "2     50.64s 1428.2655 0.6637 0.7244 0.6437 0.6708 0.7295\tNew best macro f1! Saving model and predictions.\n",
      "3     50.40s 1405.1337 0.7007 0.7279 0.6863 0.6908 0.7381\tNew best macro f1! Saving model and predictions.\n",
      "4     50.25s 1386.3075 0.7183 0.7244 0.7043 0.6940 0.7366\tNew best macro f1! Saving model and predictions.\n",
      "5     50.17s 1380.5710 0.7295 0.7372 0.7147 0.7070 0.7486\tNew best macro f1! Saving model and predictions.\n",
      "7     50.48s 1375.1205 0.7355 0.7395 0.7202 0.7096 0.7508\tNew best macro f1! Saving model and predictions.\n",
      "10    50.27s 1371.1011 0.7390 0.7500 0.7250 0.7211 0.7608\tNew best macro f1! Saving model and predictions.\n",
      "15    50.23s 1366.3676 0.7443 0.7500 0.7303 0.7215 0.7609\tNew best macro f1! Saving model and predictions.\n",
      "17    50.30s 1365.2417 0.7483 0.7512 0.7345 0.7221 0.7618\tNew best macro f1! Saving model and predictions.\n",
      "27    49.68s 1357.6969 0.7591 0.7523 0.7453 0.7246 0.7632\tNew best macro f1! Saving model and predictions.\n",
      "28    49.88s 1357.3611 0.7584 0.7558 0.7447 0.7273 0.7663\tNew best macro f1! Saving model and predictions.\n",
      "33    49.74s 1353.6361 0.7659 0.7651 0.7518 0.7364 0.7748\tNew best macro f1! Saving model and predictions.\n",
      "36    50.07s 1352.6644 0.7736 0.7698 0.7577 0.7379 0.7783\tNew best macro f1! Saving model and predictions.\n",
      "40    49.84s 1349.8722 0.7778 0.7709 0.7625 0.7404 0.7797\tNew best macro f1! Saving model and predictions.\n",
      "42    49.86s 1348.9561 0.7847 0.7802 0.7680 0.7481 0.7879\tNew best macro f1! Saving model and predictions.\n",
      "43    50.71s 1348.5822 0.7807 0.7884 0.7644 0.7572 0.7957\tNew best macro f1! Saving model and predictions.\n"
     ]
    }
   ],
   "source": [
    "# helpful for quick debugging. [0, len(x_train)) means no limit\n",
    "lower = 0\n",
    "upper = len(x_train)\n",
    "batch_size = 32\n",
    "lr = 3e-4\n",
    "epochs = 10000\n",
    "mlp = train(x_train[lower:upper], y_train[lower:upper], x_test, y_test, len(vocab), batch_size, lr, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
