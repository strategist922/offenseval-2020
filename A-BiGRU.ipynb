{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available! Using device 0 (GeForce GTX 1070)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn import metrics as skmetrics\n",
    "import torch\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "embedding_dim = 25\n",
    "embedding_file = 'data/glove.twitter.27B/glove.twitter.27B.%dd.txt' % embedding_dim\n",
    "olid_data = 'data/OLIDv1.0/olid-training-v1.0.tsv'\n",
    "olid_hashtags = 'data/olid_segmentations.tsv'\n",
    "if torch.cuda.is_available():\n",
    "    device = 0 \n",
    "    print('CUDA available! Using device %d (%s)' % (device, torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    device = None\n",
    "    print('CUDA unavailable! Using CPU.')\n",
    "\n",
    "np.random.seed(1234) # helps reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    # https://discuss.pytorch.org/t/is-there-something-like-keras-utils-to-categorical-in-pytorch/5960\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "def report(y, y_hat, metrics=['accuracy', 'precision', 'recall', 'f1-weighted', 'f1-macro']):\n",
    "    results = []\n",
    "    metrics = metrics.copy()\n",
    "    while len(metrics) > 0:\n",
    "        m = metrics.pop(0)\n",
    "        if m == 'accuracy':\n",
    "            results.append(skmetrics.accuracy_score(y, y_hat))\n",
    "        elif m == 'precision':\n",
    "            results.append(skmetrics.precision_score(y, y_hat))\n",
    "        elif m == 'recall':\n",
    "            results.append(skmetrics.recall_score(y, y_hat))\n",
    "        elif m == 'f1-weighted':\n",
    "            results.append(skmetrics.f1_score(y, y_hat, average='weighted'))\n",
    "        elif m == 'f1-macro':\n",
    "            results.append(skmetrics.f1_score(y, y_hat, average='macro'))\n",
    "        else:\n",
    "            print('Metric unknown: %s' % m)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data in 3.73s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "# Load tweets and labels\n",
    "with open(olid_data, encoding='utf-8') as f:\n",
    "    raw = csv.reader(f, delimiter='\\t')\n",
    "    x_raw = []\n",
    "    y = []\n",
    "    for r in raw:\n",
    "        x_raw.append(r[1])\n",
    "        y.append(0 if r[2] == 'NOT' else 1)\n",
    "    x_raw = x_raw[1:]\n",
    "    y = np.array(y[1:])\n",
    "    bad_words = [row[:-1] for row in f.readlines()[1:]]\n",
    "    \n",
    "# Load hashtag segmentations\n",
    "segmentations = {}\n",
    "for line in open(olid_hashtags, encoding='utf-8'):\n",
    "    terms = [x.strip().lower() for x in line.split('\\t')]\n",
    "    hashtag, segmentation = terms[0], terms[1]\n",
    "    segmentations[hashtag] = segmentation\n",
    "\n",
    "# Tokenize data\n",
    "tokenizer = TweetTokenizer(preserve_case=False)  \n",
    "x = []\n",
    "vocab = {}\n",
    "i = 1 # start from 1. 0 is pad.\n",
    "for tweet in x_raw:\n",
    "    example = []\n",
    "    for token in tokenizer.tokenize(tweet):\n",
    "        # if it's a hashtag, look up segmentaion\n",
    "        if token[0] == '#' and token[1:] in segmentations:\n",
    "            sequence = segmentations[token[1:]].split()\n",
    "        else:\n",
    "            sequence = [token]\n",
    "            \n",
    "        for word in sequence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = i\n",
    "                i += 1\n",
    "            example.append(vocab[word])\n",
    "    x.append(example)\n",
    "    \n",
    "# Randomly shuffle\n",
    "i = np.arange(len(x))  \n",
    "np.random.shuffle(i)\n",
    "x = [torch.LongTensor(x[k]).to(device) for k in i]\n",
    "y = torch.FloatTensor(to_categorical(y[i], 2)).to(device)\n",
    "\n",
    "split = 0.7\n",
    "split_index = int(len(x) * split)\n",
    "x_train = x[:split_index]\n",
    "y_train = y[:split_index]\n",
    "x_val = x[split_index:]\n",
    "y_val = y[split_index:]\n",
    "print('Loaded data in %.2fs' % (time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings in 15.50s\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "# On my mac, GloVe loads 25D in 30s, 50D in 100s, 100D in 630s\n",
    "start = time()\n",
    "embeddings = {}\n",
    "with open(embedding_file, encoding='utf-8') as f:\n",
    "    raw = [row.split() for row in f.readlines()]\n",
    "    for r in raw:\n",
    "        embeddings[r[0]] = np.array([float(v) for v in r[1:]])\n",
    "\n",
    "# Create embedding weight matrix that corresponds to the ids we've already set for x\n",
    "# https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "embedding_weights = np.zeros((len(vocab) + 1, embedding_dim)) # add 1 to account for pad\n",
    "for word, i in vocab.items():\n",
    "    try: \n",
    "        embedding_weights[i] = embeddings[word]\n",
    "    except KeyError:\n",
    "        embedding_weights[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "embedding_weights = torch.FloatTensor(embedding_weights)\n",
    "print('Loaded embeddings in %.2fs' % (time() - start)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings=None, dim_emb=25, n_classes=2, device=0):\n",
    "        super(GRU, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_classes = n_classes\n",
    "        self.device = device\n",
    "        \n",
    "        bidirectional = False\n",
    "        gru_size = 24\n",
    "        if embeddings is None:\n",
    "            self.dim_emb = dim_emb\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.dim_emb)\n",
    "        else:\n",
    "            self.dim_emb = embeddings.shape[1]\n",
    "            self.embedding = nn.Embedding.from_pretrained(embeddings) # defaults to frozen\n",
    "        self.gru = nn.GRU(input_size=self.dim_emb,\n",
    "                          hidden_size=gru_size,\n",
    "                          num_layers=2,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=bidirectional,\n",
    "                          dropout=0.2\n",
    "                         )\n",
    "        if bidirectional:\n",
    "            self.fc1 = nn.Linear(2 * gru_size, self.n_classes)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(gru_size, self.n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, h = self.gru(x)\n",
    "        x = h.permute(1, 0, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(out[:, -1])\n",
    "        y = self.softmax(self.fc1(x))\n",
    "        return y\n",
    "    \n",
    "    def predict(self, x, one_hot=False, batch_size=32):\n",
    "        if one_hot:\n",
    "            y = torch.zeros((len(x), self.n_classes))\n",
    "        else:\n",
    "            y = torch.zeros((len(x)))\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            batch = rnn.pack_sequence(x[i:i+batch_size], enforce_sorted=False)\n",
    "            batch, _ = rnn.pad_packed_sequence(batch, batch_first=True)\n",
    "            logits = self.forward(batch)\n",
    "            preds = torch.max(logits, 1)[1]\n",
    "            if one_hot:\n",
    "                pass # broken\n",
    "                # y[i:i+batch_size, preds] = 1\n",
    "            else:\n",
    "                y[i:i+batch_size] = preds\n",
    "        return y\n",
    "    \n",
    "def train(x_train, y_train, x_val, y_val, vocab_size, batch_size, lr, epochs):\n",
    "    print('Start Training!')\n",
    "    mlp = GRU(vocab_size, embedding_weights)\n",
    "    if torch.cuda.is_available():\n",
    "        mlp.cuda()\n",
    "    optimizer = optim.Adam(mlp.parameters(), lr=lr)\n",
    "    max_f1 = 0\n",
    "    print('epoch time   t_loss   t_acc  v_acc  t_f1m  v_f1m  v_f1w')\n",
    "    for epoch in range(epochs):\n",
    "        start = time()\n",
    "        total_loss = 0.0\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            mlp.zero_grad()\n",
    "            batch = rnn.pack_sequence(x_train[i:i+batch_size], enforce_sorted=False)\n",
    "            batch, _ = rnn.pad_packed_sequence(batch, batch_first=True)\n",
    "            probs = mlp.forward(batch)\n",
    "            NLL = torch.neg(torch.log(probs))\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "            losses = torch.bmm(NLL.view(len(y_batch), 1, 2), y_batch.view(len(y_batch), 2, 1))\n",
    "            loss = torch.sum(losses) / len(y_batch)\n",
    "            total_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        metrics = ['accuracy', 'f1-weighted', 'f1-macro']\n",
    "        t_y_hat = mlp.predict(x_train, False, 1024)\n",
    "        t_dense = torch.max(y_train, 1)[1]\n",
    "        t_acc, t_f1w, t_f1m = report(t_dense.cpu(), t_y_hat, metrics=metrics)\n",
    "        v_y_hat = mlp.predict(x_val, False, 1024)\n",
    "        v_dense = torch.max(y_val, 1)[1]\n",
    "        v_acc, v_f1w, v_f1m = report(v_dense.cpu(), v_y_hat, metrics=metrics)\n",
    "        print('\\r%-5d %5.2fs %08.4f %.4f %.4f %.4f %.4f %.4f' % \n",
    "              (epoch, time() - start, total_loss, t_acc, v_acc, t_f1m, v_f1m, v_f1w), end='')\n",
    "        if v_f1m > max_f1:\n",
    "             print('\\tNew best macro f1! Saving model.')\n",
    "             torch.save(mlp.state_dict(), 'models/best-bigru.model' % epoch)\n",
    "             max_f1 = v_f1m\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "epoch time   t_loss   t_acc  v_acc  t_f1m  v_f1m  v_f1w\n",
      "0     20.08s 602.8276 0.5119 0.5156 0.4915 0.4935 0.5294\tNew best macro f1! Saving model.\n",
      "1     20.10s 602.8245 0.5174 0.5164 0.4973 0.4951 0.5302\tNew best macro f1! Saving model.\n",
      "4     20.81s 602.7534 0.5144 0.5038 0.4932 0.4835 0.5182"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-71d21588972c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3e-4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-a53d8fed4c3a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(x_train, y_train, x_val, y_val, vocab_size, batch_size, lr, epochs)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m             \u001b[0mNLL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-a53d8fed4c3a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[1;32m--> 716\u001b[1;33m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    717\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# helpful for quick debugging. [0, len(x_train)) means no limit\n",
    "lower = 0\n",
    "upper = len(x_train)\n",
    "batch_size = 64\n",
    "lr = 3e-4\n",
    "epochs = 10000\n",
    "mlp = train(x_train[lower:upper], y_train[lower:upper], x_val, y_val, len(vocab), batch_size, lr, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
