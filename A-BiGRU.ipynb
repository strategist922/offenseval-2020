{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA unavailable! Using CPU.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "use_segmentation = False\n",
    "embedding_dim = 25\n",
    "embedding_file = 'data/glove.twitter.27B/glove.twitter.27B.%dd.txt' % embedding_dim\n",
    "olid_data = 'data/OLIDv1.0/olid-training-v1.0.tsv'\n",
    "olid_hashtags = 'data/olid_segmentations.tsv'\n",
    "if torch.cuda.is_available():\n",
    "    device = 0\n",
    "    print('CUDA available! Using device %d (%s)' % (device, torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    device = None\n",
    "    print('CUDA unavailable! Using CPU.')\n",
    "\n",
    "np.random.seed(1234) # helps reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    # https://discuss.pytorch.org/t/is-there-something-like-keras-utils-to-categorical-in-pytorch/5960\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "def report(y, y_hat, metrics=['accuracy', 'precision', 'recall', 'f1-weighted', 'f1-macro']):\n",
    "    results = []\n",
    "    metrics = metrics.copy()\n",
    "    while len(metrics) > 0:\n",
    "        m = metrics.pop(0)\n",
    "        if m == 'accuracy':\n",
    "            results.append(skmetrics.accuracy_score(y, y_hat))\n",
    "        elif m == 'precision':\n",
    "            results.append(skmetrics.precision_score(y, y_hat))\n",
    "        elif m == 'recall':\n",
    "            results.append(skmetrics.recall_score(y, y_hat))\n",
    "        elif m == 'f1-weighted':\n",
    "            results.append(skmetrics.f1_score(y, y_hat, average='weighted'))\n",
    "        elif m == 'f1-macro':\n",
    "            results.append(skmetrics.f1_score(y, y_hat, average='macro'))\n",
    "        else:\n",
    "            print('Metric unknown: %s' % m)\n",
    "    return results\n",
    "\n",
    "# https://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console\n",
    "def printProgressBar(iteration, total, prefix='', suffix='', decimals=1, length=100, fill='█'):\n",
    "    '''\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "    '''\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end='\\r')\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data in 2.27s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "# Load tweets and labels\n",
    "with open(olid_data, encoding='utf-8') as f:\n",
    "    raw = csv.reader(f, delimiter='\\t')\n",
    "    x_raw = []\n",
    "    y = []\n",
    "    for r in raw:\n",
    "        x_raw.append(r[1])\n",
    "        y.append(0 if r[2] == 'NOT' else 1)\n",
    "    x_raw = x_raw[1:]\n",
    "    y = np.array(y[1:])\n",
    "    bad_words = [row[:-1] for row in f.readlines()[1:]]\n",
    "    \n",
    "# Load hashtag segmentations\n",
    "segmentations = {}\n",
    "for line in open(olid_hashtags):\n",
    "    terms = [x.strip() for x in line.split('\\t')]\n",
    "    hashtag, segmentation = terms[0], terms[1]\n",
    "    segmentations[hashtag] = segmentation\n",
    "\n",
    "# Tokenize data\n",
    "tokenizer = TweetTokenizer(preserve_case=False)  \n",
    "x = []\n",
    "vocab = {}\n",
    "i = 0\n",
    "for tweet in x_raw:\n",
    "    example = []\n",
    "    for token in tokenizer.tokenize(tweet):\n",
    "        # if it's a hashtag, look up segmentaion\n",
    "        if use_segmentation and token[0] == '#' and token[1:] in segmentations:\n",
    "            sequence = segmentations[token[1:]].split()\n",
    "        else:\n",
    "            sequence = [token]\n",
    "            \n",
    "        for word in sequence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = i\n",
    "                i += 1\n",
    "            example.append(vocab[word])\n",
    "    x.append(example)\n",
    "    \n",
    "#Randomly shuffle\n",
    "i = np.arange(len(x))\n",
    "np.random.shuffle(i)\n",
    "x = [torch.LongTensor(x[k]).to(device) for k in i]\n",
    "# y = torch.FloatTensor(to_categorical(y[i], 2)).to(device)\n",
    "y = torch.IntTensor(y[i]).to(device)\n",
    "\n",
    "split = 0.7\n",
    "split_index = int(len(x) * split)\n",
    "x_train = x[:split_index]\n",
    "y_train = y[:split_index]\n",
    "x_val = x[split_index:]\n",
    "y_val = y[split_index:]\n",
    "print('Loaded data in %.2fs' % (time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings in 54.49s\n",
      "Build embedding matrix in 0.09s\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "# On my mac, GloVe loads 25D in 30s, 50D in 100s, 100D in 630s\n",
    "start = time()\n",
    "embeddings = {}\n",
    "with open(embedding_file) as f:\n",
    "    raw = [row.split() for row in f.readlines()]\n",
    "    for r in raw:\n",
    "        embeddings[r[0]] = np.array([float(v) for v in r[1:]])\n",
    "\n",
    "# Create embedding weight matrix that corresponds to the ids we've already set for x\n",
    "# https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "start = time()\n",
    "embedding_weights = np.zeros((len(vocab), embedding_dim))\n",
    "for word, i in vocab.items():\n",
    "    try: \n",
    "        embedding_weights[i] = embeddings[word]\n",
    "    except KeyError:\n",
    "        embedding_weights[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "embedding_weights = torch.FloatTensor(embedding_weights).to(device)\n",
    "print('Loaded embeddings in %.2fs' % (time() - start)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings=None, dim_emb=10, n_classes=2, device=0):\n",
    "        super(GRU, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_classes = n_classes\n",
    "        self.device = device\n",
    "        \n",
    "        if embeddings is None:\n",
    "            self.dim_emb = dim_emb\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.dim_emb)\n",
    "        else:\n",
    "            self.dim_emb = embeddings.shape[1]\n",
    "            self.embedding = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.gru = nn.GRU(input_size=self.dim_emb,\n",
    "                          hidden_size=16,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=True\n",
    "                         )\n",
    "        self.fc1 = nn.Linear(2*16, 128)\n",
    "        self.fc2 = nn.Linear(128, self.n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x, train=False):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(1, x.shape[0], x.shape[1]) # batch of 1\n",
    "        x, h_n = self.gru(x)\n",
    "        x = torch.flatten(h_n)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        y = self.softmax(x)\n",
    "        return y\n",
    "    \n",
    "    def predict(self, x, one_hot=False):\n",
    "        if one_hot:\n",
    "            y = np.zeros((len(x), self.n_classes))\n",
    "        else:\n",
    "            y = np.zeros((len(x)))\n",
    "            \n",
    "        for i in range(len(x)):\n",
    "            logits = self.forward(x[i], train=False)\n",
    "            pred = torch.argmax(logits)\n",
    "            if one_hot:\n",
    "                y[i, pred] = 1\n",
    "            else:\n",
    "                y[i] = pred\n",
    "        return y\n",
    "    \n",
    "def train(x_train, y_train, x_val, y_val, vocab_size, epochs):\n",
    "    print('Start Training!')\n",
    "    mlp = GRU(vocab_size, embedding_weights)\n",
    "    if torch.cuda.is_available():\n",
    "        mlp.cuda()\n",
    "    optimizer = optim.Adam(mlp.parameters(), lr=0.001)\n",
    "    batch_size = 1\n",
    "    max_f1 = 0\n",
    "    for epoch in range(epochs):\n",
    "        print('-------------')\n",
    "        print('Epoch %d' % epoch)\n",
    "        start = time()\n",
    "        total_loss = 0.0\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            mlp.zero_grad()\n",
    "            probs = mlp.forward(x_train[i])\n",
    "            onehot = torch.zeros(2)\n",
    "            onehot[y_train[i]] = 1\n",
    "            loss = torch.neg(torch.log(probs)).dot(onehot)            \n",
    "            total_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 2 == 0:\n",
    "                p = '%d/%d' % (i+2, len(x_train))\n",
    "                printProgressBar(i+2, len(x_train), prefix=p, length=60)\n",
    "        print('loss: %.4f' % total_loss)\n",
    "        print('time: %.2fs' % (time() - start))\n",
    "        metrics = ['accuracy', 'f1-weighted', 'f1-macro']\n",
    "        t_acc, t_f1w, t_f1m = report(y_train, mlp.predict(x_train), metrics=metrics)\n",
    "        v_acc, v_f1w, v_f1m = report(y_val, mlp.predict(x_val), metrics=metrics)\n",
    "        print('train_acc: %.4f   f1_weighted: %.4f  f1_macro: %.4f' % (t_acc, t_f1w, t_f1m))\n",
    "        print('val_acc:   %.4f   f1_weighted: %.4f  f1_macro: %.4f' % (v_acc, v_f1w, v_f1m))\n",
    "        if v_f1m > max_f1:\n",
    "            print('New best macro f1! Saving model.')\n",
    "            torch.save(mlp.state_dict(), 'models/best-epoch%d.model' % epoch)\n",
    "            max_f1 = v_f1m\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "-------------\n",
      "Epoch 0\n",
      "114/300 |██████████████████████--------------------------------------| 38.0% \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-202-1f3eedc31f92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m \u001b[0;31m# len(x_train) # helpful for quick debugging. len(x_train) means no limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-201-cc6503a59022>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x_train, y_train, x_val, y_val, vocab_size, epochs)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "limit = 300 # len(x_train) # helpful for quick debugging. len(x_train) means no limit\n",
    "epochs = 2\n",
    "mlp = train(x_train[:limit], y_train[:limit], x_val, y_val, len(vocab), epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
