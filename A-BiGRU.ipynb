{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available! Using device 0 (GeForce GTX 1070)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn import metrics as skmetrics\n",
    "import torch\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "use_segmentation = False\n",
    "embedding_dim = 25\n",
    "embedding_file = 'data/glove.twitter.27B/glove.twitter.27B.%dd.txt' % embedding_dim\n",
    "olid_data = 'data/OLIDv1.0/olid-training-v1.0.tsv'\n",
    "olid_hashtags = 'data/olid_segmentations.tsv'\n",
    "if torch.cuda.is_available():\n",
    "    device = 0 \n",
    "    print('CUDA available! Using device %d (%s)' % (device, torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    device = None\n",
    "    print('CUDA unavailable! Using CPU.')\n",
    "\n",
    "np.random.seed(1234) # helps reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    # https://discuss.pytorch.org/t/is-there-something-like-keras-utils-to-categorical-in-pytorch/5960\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "def report(y, y_hat, metrics=['accuracy', 'precision', 'recall', 'f1-weighted', 'f1-macro']):\n",
    "    results = []\n",
    "    metrics = metrics.copy()\n",
    "    while len(metrics) > 0:\n",
    "        m = metrics.pop(0)\n",
    "        if m == 'accuracy':\n",
    "            results.append(skmetrics.accuracy_score(y, y_hat))\n",
    "        elif m == 'precision':\n",
    "            results.append(skmetrics.precision_score(y, y_hat))\n",
    "        elif m == 'recall':\n",
    "            results.append(skmetrics.recall_score(y, y_hat))\n",
    "        elif m == 'f1-weighted':\n",
    "            results.append(skmetrics.f1_score(y, y_hat, average='weighted'))\n",
    "        elif m == 'f1-macro':\n",
    "            results.append(skmetrics.f1_score(y, y_hat, average='macro'))\n",
    "        else:\n",
    "            print('Metric unknown: %s' % m)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data in 3.52s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "# Load tweets and labels\n",
    "with open(olid_data, encoding='utf-8') as f:\n",
    "    raw = csv.reader(f, delimiter='\\t')\n",
    "    x_raw = []\n",
    "    y = []\n",
    "    for r in raw:\n",
    "        x_raw.append(r[1])\n",
    "        y.append(0 if r[2] == 'NOT' else 1)\n",
    "    x_raw = x_raw[1:]\n",
    "    y = np.array(y[1:])\n",
    "    bad_words = [row[:-1] for row in f.readlines()[1:]]\n",
    "    \n",
    "# Load hashtag segmentations\n",
    "segmentations = {}\n",
    "for line in open(olid_hashtags, encoding='utf-8'):\n",
    "    terms = [x.strip() for x in line.split('\\t')]\n",
    "    hashtag, segmentation = terms[0], terms[1]\n",
    "    segmentations[hashtag] = segmentation\n",
    "\n",
    "# Tokenize data\n",
    "tokenizer = TweetTokenizer(preserve_case=False)  \n",
    "x = []\n",
    "vocab = {}\n",
    "i = 1 # start from 1. 0 is pad.\n",
    "for tweet in x_raw:\n",
    "    example = []\n",
    "    for token in tokenizer.tokenize(tweet):\n",
    "        # if it's a hashtag, look up segmentaion\n",
    "        if use_segmentation and token[0] == '#' and token[1:] in segmentations:\n",
    "            sequence = segmentations[token[1:]].split()\n",
    "        else:\n",
    "            sequence = [token]\n",
    "            \n",
    "        for word in sequence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = i\n",
    "                i += 1\n",
    "            example.append(vocab[word])\n",
    "    x.append(example)\n",
    "    \n",
    "# Randomly shuffle\n",
    "# As the result of a 3 hour long bug hunt,\n",
    "# we have to subtract 1 because cudnn throws a fit if it tries to access\n",
    "# the example at the end of the file. Some illegal memory access error?\n",
    "# This is wack and I'm mad.\n",
    "i = np.arange(len(x) - 1)  \n",
    "np.random.shuffle(i)\n",
    "x = [torch.LongTensor(x[k]).to(device) for k in i]\n",
    "y = torch.FloatTensor(to_categorical(y[i], 2)).to(device)\n",
    "\n",
    "split = 0.7\n",
    "split_index = int(len(x) * split)\n",
    "x_train = x[:split_index]\n",
    "y_train = y[:split_index]\n",
    "x_val = x[split_index:]\n",
    "y_val = y[split_index:]\n",
    "print('Loaded data in %.2fs' % (time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings in 15.22s\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "# On my mac, GloVe loads 25D in 30s, 50D in 100s, 100D in 630s\n",
    "start = time()\n",
    "embeddings = {}\n",
    "with open(embedding_file, encoding='utf-8') as f:\n",
    "    raw = [row.split() for row in f.readlines()]\n",
    "    for r in raw:\n",
    "        embeddings[r[0]] = np.array([float(v) for v in r[1:]])\n",
    "\n",
    "# Create embedding weight matrix that corresponds to the ids we've already set for x\n",
    "# https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "embedding_weights = np.zeros((len(vocab) + 1, embedding_dim)) # add 1 to account for pad\n",
    "for word, i in vocab.items():\n",
    "    try: \n",
    "        embedding_weights[i] = embeddings[word]\n",
    "    except KeyError:\n",
    "        embedding_weights[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "embedding_weights = torch.FloatTensor(embedding_weights)\n",
    "print('Loaded embeddings in %.2fs' % (time() - start)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings=None, dim_emb=25, n_classes=2, device=0):\n",
    "        super(GRU, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_classes = n_classes\n",
    "        self.device = device\n",
    "        \n",
    "        gru_size = 128\n",
    "        fc_size = 1024\n",
    "        if embeddings is None:\n",
    "            self.dim_emb = dim_emb\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.dim_emb)\n",
    "        else:\n",
    "            self.dim_emb = embeddings.shape[1]\n",
    "            self.embedding = nn.Embedding.from_pretrained(embeddings) # defaults to frozen\n",
    "        self.gru = nn.GRU(input_size=self.dim_emb,\n",
    "                          hidden_size=gru_size,\n",
    "                          num_layers=2,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=True\n",
    "                         )\n",
    "        self.fc1 = nn.Linear(2 * gru_size, self.n_classes) #fc_size)\n",
    "        self.fc2 = nn.Linear(fc_size, fc_size)\n",
    "        self.fc3 = nn.Linear(fc_size, self.n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, h = self.gru(x)\n",
    "        x = h.permute(1, 0, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(out[:, -1])\n",
    "        # x = self.relu(self.fc1(x))\n",
    "        # x = self.relu(self.fc2(x))\n",
    "        y = self.softmax(self.fc1(x))\n",
    "        return y\n",
    "    \n",
    "    def predict(self, x, one_hot=False, batch_size=32):\n",
    "        if one_hot:\n",
    "            y = torch.zeros((len(x), self.n_classes))\n",
    "        else:\n",
    "            y = torch.zeros((len(x)))\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            batch = rnn.pack_sequence(x[i:i+batch_size], enforce_sorted=False)\n",
    "            batch, _ = rnn.pad_packed_sequence(batch, batch_first=True)\n",
    "            logits = self.forward(batch)\n",
    "            preds = torch.max(logits, 1)[1]\n",
    "            if one_hot:\n",
    "                pass # broken\n",
    "                # y[i:i+batch_size, preds] = 1\n",
    "            else:\n",
    "                y[i:i+batch_size] = preds\n",
    "        return y\n",
    "    \n",
    "def train(x_train, y_train, x_val, y_val, vocab_size, batch_size, lr, epochs):\n",
    "    print('Start Training!')\n",
    "    mlp = GRU(vocab_size, embedding_weights)\n",
    "    if torch.cuda.is_available():\n",
    "        mlp.cuda()\n",
    "    optimizer = optim.Adam(mlp.parameters(), lr=lr)\n",
    "    max_f1 = 0\n",
    "    print('epoch time  t_loss   t_acc  v_acc  t_f1m  v_f1m  v_f1w')\n",
    "    for epoch in range(epochs):\n",
    "        start = time()\n",
    "        total_loss = 0.0\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            mlp.zero_grad()\n",
    "            batch = rnn.pack_sequence(x_train[i:i+batch_size], enforce_sorted=False)\n",
    "            batch, _ = rnn.pad_packed_sequence(batch, batch_first=True)\n",
    "            probs = mlp.forward(batch)\n",
    "            NLL = torch.neg(torch.log(probs))\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "            losses = torch.bmm(NLL.view(len(y_batch), 1, 2), y_batch.view(len(y_batch), 2, 1))\n",
    "            loss = torch.sum(losses) / len(y_batch)\n",
    "            total_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        metrics = ['accuracy', 'f1-weighted', 'f1-macro']\n",
    "        t_y_hat = mlp.predict(x_train, False, 1024)\n",
    "        t_dense = torch.max(y_train, 1)[1]\n",
    "        t_acc, t_f1w, t_f1m = report(t_dense.cpu(), t_y_hat, metrics=metrics)\n",
    "        v_y_hat = mlp.predict(x_val, False, 1024)\n",
    "        v_dense = torch.max(y_val, 1)[1]\n",
    "        v_acc, v_f1w, v_f1m = report(v_dense.cpu(), v_y_hat, metrics=metrics)\n",
    "        print('\\r%-5d %.2fs %08.4f %.4f %.4f %.4f %.4f %.4f' % \n",
    "              (epoch, time() - start, total_loss, t_acc, v_acc, t_f1m, v_f1m, v_f1w), end='')\n",
    "        if v_f1m > max_f1:\n",
    "             print('\\tNew best macro f1! Saving model.')\n",
    "             torch.save(mlp.state_dict(), 'models/best-bigru-f1%d.model' % epoch)\n",
    "             max_f1 = v_f1m\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "epoch time  t_loss   t_acc  v_acc  t_f1m  v_f1m  v_f1w\n",
      "0     8.87s 116.2137 0.3358 0.3311 0.2563 0.2501 0.1668\tNew best macro f1! Saving model.\n",
      "2     8.61s 116.2036 0.3777 0.3313 0.3464 0.2503 0.1669\tNew best macro f1! Saving model.\n",
      "7     8.39s 116.1938 0.4170 0.3318 0.4106 0.2511 0.1679\tNew best macro f1! Saving model.\n",
      "8     8.45s 116.1900 0.4504 0.3338 0.4504 0.2566 0.1756\tNew best macro f1! Saving model.\n",
      "10    8.36s 116.1816 0.4799 0.3361 0.4757 0.2617 0.1824\tNew best macro f1! Saving model.\n",
      "11    8.41s 116.1822 0.5371 0.4068 0.5049 0.3973 0.3716\tNew best macro f1! Saving model.\n",
      "13    8.31s 116.1742 0.5618 0.4678 0.5124 0.4650 0.4780\tNew best macro f1! Saving model.\n",
      "14    8.28s 116.1451 0.6295 0.6259 0.5511 0.5494 0.6122\tNew best macro f1! Saving model.\n",
      "15    8.37s 115.9445 0.6136 0.6062 0.5726 0.5657 0.6106\tNew best macro f1! Saving model.\n",
      "16    8.36s 115.6773 0.6477 0.6382 0.6223 0.6114 0.6459\tNew best macro f1! Saving model.\n",
      "17    8.30s 115.0728 0.6962 0.6908 0.6706 0.6633 0.6959\tNew best macro f1! Saving model.\n",
      "18    8.35s 114.6656 0.7240 0.7178 0.6877 0.6765 0.7156\tNew best macro f1! Saving model.\n",
      "19    8.36s 114.3556 0.7327 0.7178 0.7006 0.6804 0.7174\tNew best macro f1! Saving model.\n",
      "20    8.38s 114.0851 0.7369 0.7170 0.7117 0.6878 0.7201\tNew best macro f1! Saving model.\n",
      "21    8.38s 113.8393 0.7416 0.7218 0.7193 0.6954 0.7258\tNew best macro f1! Saving model.\n",
      "22    8.31s 113.6159 0.7537 0.7364 0.7305 0.7086 0.7391\tNew best macro f1! Saving model.\n",
      "23    8.33s 113.4664 0.7590 0.7440 0.7355 0.7157 0.7460\tNew best macro f1! Saving model.\n",
      "24    8.45s 113.3585 0.7627 0.7470 0.7393 0.7186 0.7488\tNew best macro f1! Saving model.\n",
      "25    8.36s 113.2806 0.7640 0.7465 0.7411 0.7187 0.7486\tNew best macro f1! Saving model.\n",
      "26    8.39s 113.2173 0.7654 0.7477 0.7426 0.7203 0.7499\tNew best macro f1! Saving model.\n",
      "34    8.43s 112.7142 0.7754 0.7475 0.7548 0.7207 0.7499\tNew best macro f1! Saving model.\n",
      "80    8.77s 109.4608 0.8758 0.7110 0.8660 0.6865 0.7161"
     ]
    }
   ],
   "source": [
    "# helpful for quick debugging. [0, len(x_train)) means no limit\n",
    "lower = 0\n",
    "upper = len(x_train)\n",
    "batch_size = 128\n",
    "lr = 3e-4\n",
    "epochs = 10000\n",
    "mlp = train(x_train[lower:upper], y_train[lower:upper], x_val, y_val, len(vocab), batch_size, lr, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
