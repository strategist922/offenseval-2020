{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available! Using device 0 (GeForce GTX 1070)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn import metrics as skmetrics\n",
    "import torch\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "use_segmentation = False\n",
    "embedding_dim = 25\n",
    "embedding_file = 'data/glove.twitter.27B/glove.twitter.27B.%dd.txt' % embedding_dim\n",
    "olid_data = 'data/OLIDv1.0/olid-training-v1.0.tsv'\n",
    "olid_hashtags = 'data/olid_segmentations.tsv'\n",
    "if torch.cuda.is_available():\n",
    "    device = 0 \n",
    "    print('CUDA available! Using device %d (%s)' % (device, torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    device = None\n",
    "    print('CUDA unavailable! Using CPU.')\n",
    "\n",
    "np.random.seed(1234) # helps reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    # https://discuss.pytorch.org/t/is-there-something-like-keras-utils-to-categorical-in-pytorch/5960\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "def report(y, y_hat, metrics=['accuracy', 'precision', 'recall', 'f1-weighted', 'f1-macro']):\n",
    "    results = []\n",
    "    metrics = metrics.copy()\n",
    "    while len(metrics) > 0:\n",
    "        m = metrics.pop(0)\n",
    "        if m == 'accuracy':\n",
    "            results.append(skmetrics.accuracy_score(y, y_hat))\n",
    "        elif m == 'precision':\n",
    "            results.append(skmetrics.precision_score(y, y_hat))\n",
    "        elif m == 'recall':\n",
    "            results.append(skmetrics.recall_score(y, y_hat))\n",
    "        elif m == 'f1-weighted':\n",
    "            results.append(skmetrics.f1_score(y, y_hat, average='weighted'))\n",
    "        elif m == 'f1-macro':\n",
    "            results.append(skmetrics.f1_score(y, y_hat, average='macro'))\n",
    "        else:\n",
    "            print('Metric unknown: %s' % m)\n",
    "    return results\n",
    "\n",
    "# https://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console\n",
    "def printProgressBar(iteration, total, prefix='', suffix='', decimals=1, length=100, fill='â–ˆ'):\n",
    "    '''\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "    '''\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end='\\r')\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data in 3.72s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "# Load tweets and labels\n",
    "with open(olid_data, encoding='utf-8') as f:\n",
    "    raw = csv.reader(f, delimiter='\\t')\n",
    "    x_raw = []\n",
    "    y = []\n",
    "    for r in raw:\n",
    "        x_raw.append(r[1])\n",
    "        y.append(0 if r[2] == 'NOT' else 1)\n",
    "    x_raw = x_raw[1:]\n",
    "    y = np.array(y[1:])\n",
    "    bad_words = [row[:-1] for row in f.readlines()[1:]]\n",
    "    \n",
    "# Load hashtag segmentations\n",
    "segmentations = {}\n",
    "for line in open(olid_hashtags, encoding='utf-8'):\n",
    "    terms = [x.strip() for x in line.split('\\t')]\n",
    "    hashtag, segmentation = terms[0], terms[1]\n",
    "    segmentations[hashtag] = segmentation\n",
    "\n",
    "# Tokenize data\n",
    "tokenizer = TweetTokenizer(preserve_case=False)  \n",
    "x = []\n",
    "vocab = {}\n",
    "i = 1 # start from 1. 0 is pad.\n",
    "for tweet in x_raw:\n",
    "    example = []\n",
    "    for token in tokenizer.tokenize(tweet):\n",
    "        # if it's a hashtag, look up segmentaion\n",
    "        if use_segmentation and token[0] == '#' and token[1:] in segmentations:\n",
    "            sequence = segmentations[token[1:]].split()\n",
    "        else:\n",
    "            sequence = [token]\n",
    "            \n",
    "        for word in sequence:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = i\n",
    "                i += 1\n",
    "            example.append(vocab[word])\n",
    "    x.append(example)\n",
    "    \n",
    "# Randomly shuffle\n",
    "# As the result of a 3 hour long bug hunt,\n",
    "# we have to subtract 1 because cudnn throws a fit if it tries to access\n",
    "# the example at the end of the file. Some illegal memory access error?\n",
    "# This is wack and I'm mad.\n",
    "i = np.arange(len(x) - 1)  \n",
    "np.random.shuffle(i)\n",
    "x = [torch.LongTensor(x[k]).to(device) for k in i]\n",
    "y = torch.FloatTensor(to_categorical(y[i], 2)).to(device)\n",
    "\n",
    "split = 0.7\n",
    "split_index = int(len(x) * split)\n",
    "x_train = x[:split_index]\n",
    "y_train = y[:split_index]\n",
    "x_val = x[split_index:]\n",
    "y_val = y[split_index:]\n",
    "print('Loaded data in %.2fs' % (time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings in 15.37s\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "# On my mac, GloVe loads 25D in 30s, 50D in 100s, 100D in 630s\n",
    "start = time()\n",
    "embeddings = {}\n",
    "with open(embedding_file, encoding='utf-8') as f:\n",
    "    raw = [row.split() for row in f.readlines()]\n",
    "    for r in raw:\n",
    "        embeddings[r[0]] = np.array([float(v) for v in r[1:]])\n",
    "\n",
    "# Create embedding weight matrix that corresponds to the ids we've already set for x\n",
    "# https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n",
    "embedding_weights = np.zeros((len(vocab) + 1, embedding_dim)) # add 1 to account for pad\n",
    "for word, i in vocab.items():\n",
    "    try: \n",
    "        embedding_weights[i] = embeddings[word]\n",
    "    except KeyError:\n",
    "        embedding_weights[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "embedding_weights = torch.HalfTensor(embedding_weights)\n",
    "print('Loaded embeddings in %.2fs' % (time() - start)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.rnn as rnn\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings=None, dim_emb=10, n_classes=2, device=0):\n",
    "        super(GRU, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_classes = n_classes\n",
    "        self.device = device\n",
    "        \n",
    "        gru_size = 256\n",
    "        if embeddings is None:\n",
    "            self.dim_emb = dim_emb\n",
    "            self.embedding = nn.Embedding(self.vocab_size, self.dim_emb)\n",
    "        else:\n",
    "            self.dim_emb = embeddings.shape[1]\n",
    "            self.embedding = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.gru = nn.GRU(input_size=self.dim_emb,\n",
    "                          hidden_size=gru_size,\n",
    "                          num_layers=1,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=True\n",
    "                         )\n",
    "        #self.fc1 = nn.Linear(gru_size, fc_size)\n",
    "        #self.fc2 = nn.Linear(fc_size, self.n_classes)\n",
    "        self.fc = nn.Linear(2 * gru_size, self.n_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, h = self.gru(x)\n",
    "        x = self.relu(out[:, -1])\n",
    "        # x = self.relu(self.fc1(x))\n",
    "        # x = self.relu(self.fc2(x))\n",
    "        x = self.fc(x)\n",
    "        y = self.softmax(x)\n",
    "        return y\n",
    "    \n",
    "    def predict(self, x, one_hot=False, batch_size=32):\n",
    "        if one_hot:\n",
    "            y = torch.zeros((len(x), self.n_classes))\n",
    "        else:\n",
    "            y = torch.zeros((len(x)))\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            batch = rnn.pack_sequence(x[i:i+batch_size], enforce_sorted=False)\n",
    "            batch, _ = rnn.pad_packed_sequence(batch, batch_first=True)\n",
    "            logits = self.forward(batch)\n",
    "            preds = torch.max(logits, 1)[1]\n",
    "            if one_hot:\n",
    "                pass # broken\n",
    "                # y[i:i+batch_size, preds] = 1\n",
    "            else:\n",
    "                y[i:i+batch_size] = preds\n",
    "        return y\n",
    "    \n",
    "def train(x_train, y_train, x_val, y_val, vocab_size, batch_size, lr, epochs):\n",
    "    print('Start Training!')\n",
    "    mlp = GRU(vocab_size, None)\n",
    "    if torch.cuda.is_available():\n",
    "        mlp.cuda()\n",
    "    optimizer = optim.Adam(mlp.parameters(), lr=lr)\n",
    "    max_f1 = 0\n",
    "    print('epoch time  t_loss   t_acc  v_acc  t_f1m  v_f1m  v_f1w')\n",
    "    for epoch in range(epochs):\n",
    "        # print('-------------')\n",
    "        # print('Epoch %d' % epoch)\n",
    "        start = time()\n",
    "        total_loss = 0.0\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            mlp.zero_grad()\n",
    "            batch = rnn.pack_sequence(x_train[i:i+batch_size], enforce_sorted=False)\n",
    "            batch, _ = rnn.pad_packed_sequence(batch, batch_first=True)\n",
    "            probs = mlp.forward(batch)\n",
    "            NLL = torch.neg(torch.log(probs))\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "            losses = torch.bmm(NLL.view(len(y_batch), 1, 2), y_batch.view(len(y_batch), 2, 1))\n",
    "            loss = torch.sum(losses) / len(y_batch)\n",
    "            total_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        metrics = ['accuracy', 'f1-weighted', 'f1-macro']\n",
    "        t_y_hat = mlp.predict(x_train, False)\n",
    "        t_dense = torch.max(y_train, 1)[1]\n",
    "        t_acc, t_f1w, t_f1m = report(t_dense.cpu(), t_y_hat, metrics=metrics)\n",
    "        v_y_hat = mlp.predict(x_val, False)\n",
    "        v_dense = torch.max(y_val, 1)[1]\n",
    "        v_acc, v_f1w, v_f1m = report(v_dense.cpu(), v_y_hat, metrics=metrics)\n",
    "        print('\\r%-5d %.2fs %08.4f %.4f %.4f %.4f %.4f %.4f' % \n",
    "              (epoch, time() - start, total_loss, t_acc, v_acc, t_f1m, v_f1m, v_f1w), end='')\n",
    "        if v_f1m > max_f1:\n",
    "             print('\\tNew best macro f1! Saving model.')\n",
    "             torch.save(mlp.state_dict(), 'models/best-f1%d.model' % epoch)\n",
    "             max_f1 = v_f1m\n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training!\n",
      "epoch time  t_loss   t_acc  v_acc  t_f1m  v_f1m  v_f1w\n",
      "0     1.21s 006.2383 0.3203 0.3203 0.2849 0.2849 0.2195\tNew best macro f1! Saving model.\n",
      "1     1.34s 006.2383 0.3223 0.3223 0.2863 0.2863 0.2205\tNew best macro f1! Saving model.\n",
      "22    1.22s 006.2381 0.3223 0.3223 0.2876 0.2876 0.2231\tNew best macro f1! Saving model.\n",
      "25    1.18s 006.2380 0.3262 0.3262 0.2929 0.2929 0.2301\tNew best macro f1! Saving model.\n",
      "40    1.14s 006.2379 0.3281 0.3281 0.2956 0.2956 0.2335\tNew best macro f1! Saving model.\n",
      "46    1.14s 006.2379 0.3281 0.3281 0.2968 0.2968 0.2360\tNew best macro f1! Saving model.\n",
      "53    1.16s 006.2378 0.3262 0.3262 0.2978 0.2978 0.2398\tNew best macro f1! Saving model.\n",
      "54    1.15s 006.2378 0.3301 0.3301 0.3041 0.3041 0.2489\tNew best macro f1! Saving model.\n",
      "55    1.16s 006.2378 0.3320 0.3320 0.3066 0.3066 0.2522\tNew best macro f1! Saving model.\n",
      "56    1.16s 006.2378 0.3340 0.3340 0.3092 0.3092 0.2555\tNew best macro f1! Saving model.\n",
      "58    1.13s 006.2378 0.3379 0.3379 0.3143 0.3143 0.2621\tNew best macro f1! Saving model.\n",
      "61    1.12s 006.2377 0.3359 0.3359 0.3148 0.3148 0.2654\tNew best macro f1! Saving model.\n",
      "62    1.12s 006.2377 0.3457 0.3457 0.3280 0.3280 0.2834\tNew best macro f1! Saving model.\n",
      "63    1.14s 006.2377 0.3516 0.3516 0.3362 0.3362 0.2947\tNew best macro f1! Saving model.\n",
      "64    1.16s 006.2377 0.3535 0.3535 0.3393 0.3393 0.2996\tNew best macro f1! Saving model.\n",
      "65    1.17s 006.2377 0.3574 0.3574 0.3448 0.3448 0.3076\tNew best macro f1! Saving model.\n"
     ]
    }
   ],
   "source": [
    "# helpful for quick debugging. [0, len(x_train)) means no limit\n",
    "lower = 0\n",
    "upper = 512\n",
    "batch_size = 512\n",
    "lr = 0.0001\n",
    "epochs = 1000\n",
    "mlp = train(x_train[lower:upper], y_train[lower:upper], x_train[lower:upper], y_train[lower:upper], len(vocab), batch_size, lr, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
